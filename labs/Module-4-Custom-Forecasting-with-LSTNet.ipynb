{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 4: Using LSTNet with Amazon SageMaker to Build a Custom Forecasting Model\n",
    "\n",
    "## Overview of Lab\n",
    "\n",
    "In this lab, you will use LSTNet with Amazon SageMaker to build, train and host a state of the art time series forecasting model.\n",
    "\n",
    "## Dataset Information and License\n",
    "\n",
    "For this lab, you will be using an open source dataset entitled [“Individual Household Electric Power Consumption”](https://archive.ics.uci.edu/ml/datasets/Individual+household+electric+power+consumption) that comes from the UCI Machine Learning Repository. Information about the dataset license can be found below.\n",
    "\n",
    "The MIT License (MIT) Copyright © [2017] Zalando SE, [https://tech.zalando.com](https://tech.zalando.com)\n",
    "\n",
    "THE SOFTWARE IS PROVIDED “AS IS”, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n",
    "\n",
    "The other dataset that will be used in this lab is the [MNIST Database of handwritten digits](http://yann.lecun.com/exdb/mnist/).\n",
    "\n",
    "## Lab Instructions\n",
    "\n",
    "To complete this lab, carefully move through this notebook, from top to bottom, making sure to read all text instructions/explanations and run each code cell in order. Also be sure to view the code cell outputs. To run each cell, step-by-step in the Jupyter notebook, click within the cell and press **SHIFT + ENTER** or choose **Run** at the top of the page. You will know that a code cell has completed running when you see a number inside the square brackets located to the left of the code cell. Alternatively, **[ ]** indicates the cell has yet to be run, and **[*]** indicates that the cell is still processing.\n",
    "\n",
    "## Table of Contents <a name=\"toc\"></a>\n",
    "1. <a href=\"#section1\">Section 1: Porting LSTNet to Amazon SageMaker</a>\n",
    "1. <a href=\"#section2\">Section 2: LSTNet Distributed Training</a>\n",
    "1. <a href=\"#section3\">Section 3: Challenge - Deploying an Endpoint Using Amazon SageMaker</a>\n",
    "\n",
    "## Section 1: Porting LSTNet to Amazon SageMaker\n",
    "<a name=\"section1\"></a>\n",
    "\n",
    "In this section, you will port the LSTNet model to be trained using Amazon Sagemaker.\n",
    "\n",
    "An LSTNet model has already been developed. There are several modules containing supporting classes stored in Amazon S3:\n",
    "\n",
    "1. `lstnet.py`\n",
    "  * The declaration of the model and forward function.\n",
    "  * The model consists of a convolutional layer, dropout, a GRU, a Skip GRU, a fully connected layer, and the parallel autoregresive component.\n",
    "\n",
    "2. `timeseriesdataset.py`\n",
    "  * Classes for loading the data: `TimeSeriesData` and `TimeSeriesDataset`.\n",
    "  * `TimeSeriesDataset` is a subclass of `gluon.data.Dataset`.\n",
    "  * It implements the `__getitem__` function which returns a time series for the given index.\n",
    "  * These classes are used to load an input file and to generate successive examples with a specified window and horizon.\n",
    "  * The window is the length of timeseries used as input data for the prediction, and the horizon is the number of time steps between the end of the window and the time at which the prediction is for.\n",
    "\n",
    "3. `lstnet_sagemaker.py`\n",
    "  * This module implements the `train()` function which is used as the entrypoint for training the model on a server.\n",
    "  * This is called by Amazon SageMaker on each host in the training cluster.\n",
    "<div style=\"text-align: right\"><a href=\"#toc\">Back to top</a></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import os\n",
    "import sagemaker\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from mxnet import gluon\n",
    "from mxnet.gluon.utils import download\n",
    "\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.mxnet import MXNet\n",
    "from sagemaker.mxnet.model import MXNetModel\n",
    "\n",
    "from IPython.display import HTML\n",
    "\n",
    "role = get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment Variables\n",
    "\n",
    "Configure the following variables for your environment:\n",
    "\n",
    "1. `bucket` - The bucket name to be used to store the training data and model artifacts.\n",
    "  * Replace *LAB_BUCKET* with the value of the **LabBucket** output in the Qwiklabs console.\n",
    "\n",
    "2. `prefix` - The folder name which is used inside the bucket.\n",
    "  * This can be left as 'lstnet'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = 'qls-2098642-8ec881fafc8b1f7e-bucket-83g56p22jn65'\n",
    "\n",
    "prefix = 'lstnet'\n",
    "\n",
    "test_bucket_prefix = '/test/'\n",
    "single_host_train_bucket_prefix = '/train/single_host/'\n",
    "\n",
    "data_dir = './data'\n",
    "data_file_path = os.path.join(data_dir,'electricity.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Data\n",
    "\n",
    "The next step is to load the electricity dataset from the `electricity.txt` file that was downloaded to the notebook automatically.\n",
    "\n",
    "* The data is normalised so each reading is between 0 and 1. This is done by dividing each column by the maximum value of the column. A column is an electricity consumption time series for a single customer.\n",
    "\n",
    "There are 321 time series of electricity consumption with 26,304 time periods in each. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(26304, 321)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(data_file_path,header=None)\n",
    "max_columns = df.max().astype(np.float64)\n",
    "\n",
    "# Normalize\n",
    "df = df/max_columns\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Training, Test and Upload to S3\n",
    "\n",
    "The first 80% of the time series data is used for training and the last 20% is used as a test set.\n",
    "\n",
    "These datasets are written to a CSV file and then uploaded to Amazon S3 to be used in training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training size 21043\n",
      "Test size 5261\n"
     ]
    }
   ],
   "source": [
    "train_frac = 0.8\n",
    "\n",
    "num_time_steps = len(df)\n",
    "split_index = int(num_time_steps*train_frac)\n",
    "train = df[0:split_index]\n",
    "print('Training size {}'.format(len(train)))\n",
    "test = df[split_index:]\n",
    "print('Test size {}'.format(len(test)))\n",
    "\n",
    "test_file_path = os.path.join(data_dir,'test.csv')\n",
    "test.to_csv(test_file_path,header=None,index=False)\n",
    "train_file_path = os.path.join(data_dir,'train.csv')\n",
    "train.to_csv(train_file_path,header=None,index=False)\n",
    "\n",
    "client = boto3.client('s3')\n",
    "client.upload_file(test_file_path, bucket, prefix + test_bucket_prefix + 'test.csv')\n",
    "client.upload_file(train_file_path, bucket, prefix + single_host_train_bucket_prefix + 'train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Locally\n",
    "\n",
    "To make sure there are no obvious bugs in the code, use the `train()` function to test locally within your notebook. This is done with 1 epoch to verify that it executed correctly. There are also some basic unit tests included in the lab.\n",
    "\n",
    "The key parameters to the `train()` function in this case are:\n",
    "\n",
    "  - `hyperparameters`: The Amazon SageMaker Hyperparameters dictionary. A dictionary of string-to-string maps.\n",
    "  - `channel_input_dirs`: A dictionary of string-to-string maps from the Amazon SageMaker algorithm input channel name to the directory containing files for that input channel.\n",
    "    - **Note:** If the Amazon SageMaker training job is run in `PIPE` mode, this dictionary will be empty.\n",
    "  - `output_data_dir`: The Amazon SageMaker output data directory. After the function returns, data written to this directory is made available in the Amazon SageMaker training job output location.\n",
    "  - `num_gpus`: The number of GPU devices available on the host this script is being executed on. As we are running on a CPU notebook instance, this number is set to zero.\n",
    "  - `num_cpus`: The number of CPU devices available on the host this script is being executed on.\n",
    "  - `hosts`: A list of hostnames in the Amazon SageMaker training job cluster.\n",
    "  - `current_host`: This host's name.\n",
    "    - It will exist in the `hosts` list as well.\n",
    "  - `kwargs`: Other keyword arguments.\n",
    "  \n",
    "**Note:** You can ignore any warnings outputted by the below code cell. On an `ml.m4.xlarge` notebook this cell takes approximately 7 minutes to complete so feel free to skip this cell and move on to the next code block. During real experimentation, you may use a larger instance type to speed up testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train file path ./data/test.csv\n",
      "Test file path ./data/test.csv\n",
      "Loading file ./data/test.csv\n",
      "Loading file ./data/test.csv\n",
      "Is it a file True\n",
      "Data length 5261\n",
      "Loading file ./data/test.csv\n",
      "Loading file ./data/test.csv\n",
      "Is it a file True\n",
      "Data length 5261\n",
      "Running on cpu(0)\n",
      "Hosts ['localhost']\n",
      "Current Host localhost\n",
      "kvstore local\n",
      "Training Start\n",
      "Epoch 0: rmse 0.27444659631985885 time 52.8886 s\n",
      "Final rmse 0.15625368402554438\n",
      "Total training time: 81.15119051933289\n",
      "Training End\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages/mxnet/gluon/block.py:345: UserWarning: save_params is deprecated. Please use save_parameters. Note that if you want load from SymbolBlock later, please use export instead. For details, see https://mxnet.incubator.apache.org/tutorials/gluon/save_load_params.html\n",
      "  warnings.warn(\"save_params is deprecated. Please use save_parameters. \"\n"
     ]
    }
   ],
   "source": [
    "from lstnet_sagemaker import train\n",
    "hyperparameters = {\n",
    "    'conv_hid' : 10,\n",
    "    'gru_hid' : 10,\n",
    "    'skip_gru_hid' : 2,\n",
    "    'skip' : 5,\n",
    "    'ar_window' : 6,\n",
    "    'window' : 24*7,\n",
    "    'horizon' : 24,\n",
    "    'learning_rate' : 0.01,\n",
    "    'clip_gradient' : 10.,\n",
    "    'batch_size' : 128,\n",
    "    'epochs' : 1\n",
    "}\n",
    "channel_input_dirs = {\n",
    "    'train':data_dir,\n",
    "    'test':data_dir\n",
    "}\n",
    "train(hyperparameters = hyperparameters,\n",
    "      input_data_config = None,\n",
    "      channel_input_dirs = channel_input_dirs,\n",
    "      output_data_dir = os.path.join(data_dir, 'output'),\n",
    "      model_dir = None,\n",
    "      num_gpus = 0,\n",
    "      num_cpus = 1,\n",
    "      hosts = ['localhost'],\n",
    "      current_host = 'localhost')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose Hyperparameters\n",
    "\n",
    "Below, a set of reasonable hyperparameters are chosen.\n",
    "\n",
    "The current number of epochs is set to 10 which takes approximately 5 minutes to run on a `ml.m4.xlarge` instance. A real world training job may take place over hundreds of epochs and could run for hours depending on complexity and compute power available.\n",
    "\n",
    "**Challenge:** Can you tweak these to make the network converge faster with a lower rmse?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    'conv_hid' : 100,\n",
    "    'gru_hid' : 100,\n",
    "    'skip_gru_hid' : 5,\n",
    "    'skip' : 24,\n",
    "    'ar_window' : 24,\n",
    "    'window' : 24*7,\n",
    "    'horizon' : 24,\n",
    "    'learning_rate' : 0.001,\n",
    "    'clip_gradient' : 10.,\n",
    "    'batch_size' : 64,\n",
    "    'epochs' : 10\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trigger the Training Job Using the Amazon SageMaker Python SDK\n",
    "\n",
    "The final step is to trigger the training job using the high-level Python SDK. A lower-level SDK is also available for more detailed control of the parameters.\n",
    "\n",
    "First, an estimator is created with `sagemaker.mxnet.MXNet`. The inputs are:\n",
    "\n",
    "  * `entry_point='lstnet_sagemaker.py'`: The module used to run the training by calling the `train()` function.\n",
    "  * `source_dir='.'`: An optional directory containing code which is copied onto the Amazon SageMaker training hosts and made available to the training script.\n",
    "  * `role=role`: The IAM role which is given to the training hosts giving them privileges such as access to the S3 bucket.\n",
    "  * `output_path='s3://{}/{}/output'.format(bucket, prefix)`: The Amazon S3 bucket to store artifacts such as the model parameters.\n",
    "  * `train_instance_count=1`: The number of hosts used for training.\n",
    "    * Using a number greater than 1 will start a cluster.\n",
    "    * To take advantage of this, the training data should be sharded.\n",
    "  * `train_instance_type='ml.p3.2xlarge'` The Amazon EC2 instance type to be used for training hosts.\n",
    "    * In this case the latest generation accelerated instance, `p3`, is chosen with a Nvidia Tesla v100 GPU.\n",
    "  * `hyperparameters=hyperparameters`: The hyperparameter dictionary made available to the `train()` function in the endpoint script.\n",
    "\n",
    "Then, the `fit()` method of the estimator is called. The parameters for this method are:\n",
    "\n",
    "  * `inputs`: A dictionary containing the URLs in S3 of the 'train' data directory and the 'test' data directory.\n",
    "  * `wait` - This is specified as `False` so the `fit()` method returns immediately after the training job is created.\n",
    "    * Go to the Amazon SageMaker console to monitor the progress of the job.\n",
    "    * Set `wait` to `True` to block and see the progress of the training job output in the notebook itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The Python 2 mxnet images will be soon deprecated and may not be supported for newer upcoming versions of the mxnet images.\n",
      "Please set the argument \"py_version='py3'\" to use the Python 3 mxnet image.\n"
     ]
    }
   ],
   "source": [
    "lstnet1 = MXNet(entry_point='lstnet_sagemaker.py',\n",
    "                source_dir='.',\n",
    "                role=role,\n",
    "                output_path='s3://{}/{}/output'.format(bucket, prefix),\n",
    "                train_instance_count=1,\n",
    "                train_instance_type='ml.p3.2xlarge',\n",
    "                hyperparameters=hyperparameters, \n",
    "                framework_version=1.2)\n",
    "\n",
    "lstnet1.fit(inputs={\n",
    "                'train': 's3://{}/{}{}'.format(bucket, prefix, single_host_train_bucket_prefix),\n",
    "                'test': 's3://{}/{}{}'.format(bucket, prefix, test_bucket_prefix)\n",
    "            },\n",
    "            wait=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** `wait` is set to `False`, this means that you will not see output from the training job in the notebook as it progresses. To view this you need to access the logs in CloudWatch via the Training Job section of the Amazon SageMaker console. By setting wait to False, you are able to initiate the training job and continue through the notebook without waiting for it to complete. The job takes approximately 9 minutes to complete with training duration of 6 minutes.\n",
    "\n",
    "Once you initiate the training job, wait until you see an output above which says `INFO:sagemaker:Creating training-job with name: sagemaker-mxnet-XXXXXXX` where XXXXXXX is a set of numbers relating to the date and time the job was created. When you see this output, you can follow the training job progress via the Amazon SageMaker console in the \"Training Jobs\" section.\n",
    "\n",
    "\n",
    "\n",
    "**Challenge:** \n",
    "1. Review the training job logs in CloudWatch and identify the hyperparameters used. How long does one epoch take to complete?\n",
    "2. Graph the GPU utilisation in CloudWatch to identify how efficient the training job is. What might reduce efficiency when running a training job?\n",
    "\n",
    "\n",
    "### Section Complete\n",
    "\n",
    "You now have successfully ported LSTNet to Amazon SageMaker. The next step is to modify it to run across multiple hosts to train faster.\n",
    "\n",
    "## Section 2: LSTNet Distributed Training\n",
    "<a name=\"section2\"></a>\n",
    "\n",
    "In this section, the LSTNet model which has been ported to use Amazon SageMaker is modified to be run with distributed training.\n",
    "\n",
    "### Overview\n",
    "\n",
    "There are three main steps required to scale the training using multiple GPUs and multiple hosts (for the purposes of this lab we will be using a single GPU and multiple CPU hosts):\n",
    "\n",
    "1. Pass the appropriate `kvstore` parameter to the Gluon trainer.\n",
    "  * This specifies how parameters are synchronised between batches.\n",
    "  * In this case, `dist_device_sync` will be used which uses a parameter server to manage multiple hosts and performs the gradient updates on the GPUs when possible.\n",
    "2. Shard the training dataset.\n",
    "  * To perform distributed cluster training, the training dataset is split into shards with at least 1 shard per host.\n",
    "  * In this case it is split into 5 shards using 5 hosts.\n",
    "  * Each host trains using only a portion of the dataset.\n",
    "  * The sharded training data is stored in Amazon S3.\n",
    "3. Split each batch into portions and copy the portions onto one GPU per portion.\n",
    "  * In this case 4 GPUs will be used.\n",
    "  * Each GPU trains on only a portion of each batch.\n",
    "  * The gradients are summed over all GPUs at the end of the batch and all GPUs (and hosts when combining with distributed) are updated.\n",
    "  * These updates are performed on the GPU when possible.\n",
    "  * You will perform the splitting, and Gluon will automatically manage the synchronizing and updates.\n",
    "\n",
    "### Split Training and Test, Shard the Training Data, and Upload to S3\n",
    "\n",
    "The first 80% of the time series is used for training and the last 20% is used as a test set.\n",
    "\n",
    "The training set is sharded sequentially into 2 parts, one for each host in the cluster.\n",
    "\n",
    "These datasets are written to a CSV file and then uploaded to Amazon S3 to be used in training.\n",
    "<div style=\"text-align: right\"><a href=\"#toc\">Back to top</a></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training size 21043\n",
      "Test size 5261\n",
      "start 0\n",
      "end 10521\n",
      "start 10521\n",
      "end 21043\n",
      "Uploading file: ./data/train_0.csv with 10521 rows\n",
      "Uploading to lstnet/train/multiple_host/train_0.csv\n",
      "Uploading file: ./data/train_1.csv with 10522 rows\n",
      "Uploading to lstnet/train/multiple_host/train_1.csv\n"
     ]
    }
   ],
   "source": [
    "single_host_train_bucket_prefix = '/train/single_host/'\n",
    "multiple_host_train_bucket_prefix = '/train/multiple_host/'\n",
    "\n",
    "splits = 2\n",
    "train_frac = 0.8\n",
    "\n",
    "num_time_steps = len(df)\n",
    "split_index = int(num_time_steps*train_frac)\n",
    "train = df[0:split_index]\n",
    "print('Training size {}'.format(len(train)))\n",
    "test = df[split_index:]\n",
    "print('Test size {}'.format(len(test)))\n",
    "\n",
    "train_sets = []\n",
    "train_len = len(train)\n",
    "train_size = int(train_len)/splits\n",
    "for i in range(0,splits):\n",
    "    start = int(i*train_size)\n",
    "    end = int((i+1)*train_size)\n",
    "    print('start {}'.format(start))\n",
    "    print('end {}'.format(end))\n",
    "    if end < (train_len-1):\n",
    "        train_sets.append(train[start:end])\n",
    "    else:\n",
    "        train_sets.append(train[start:])\n",
    "\n",
    "test_file_path = os.path.join(data_dir,'test.csv')\n",
    "test.to_csv(test_file_path,header=None,index=False)\n",
    "train_file_path = os.path.join(data_dir,'train.csv')\n",
    "train.to_csv(train_file_path,header=None,index=False)\n",
    "\n",
    "client = boto3.client('s3')\n",
    "\n",
    "for i in range(0,splits):\n",
    "    file_path = os.path.join(data_dir,'train_{}.csv'.format(i))\n",
    "    print('Uploading file: {} with {} rows'.format(file_path,len(train_sets[i])))\n",
    "    train_sets[i].to_csv(file_path,header=None,index=False)\n",
    "    s3_path = prefix + '{}train_{}.csv'.format(multiple_host_train_bucket_prefix,i)\n",
    "    print('Uploading to {}'.format(s3_path))\n",
    "    client.upload_file(file_path, bucket, s3_path)\n",
    "\n",
    "client.upload_file(test_file_path, bucket, prefix + '{}test.csv'.format(test_bucket_prefix))\n",
    "client.upload_file(train_file_path, bucket, prefix + '{}train.csv'.format(single_host_train_bucket_prefix))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modifications to `lstnet_sagemaker.py` when using multiple GPUs/hosts\n",
    "\n",
    "There are two main changes to the module:\n",
    "    \n",
    "1. Set the `kvstore` to `dist_device_sync` when multiple GPUs and hosts are available.\n",
    "1. Split each batch into one part per GPU and copy each part to a separate GPU before training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import mxnet as mx\r\n",
      "from mxnet import nd, gluon, autograd, kv\r\n",
      "import numpy as np\r\n",
      "from mxnet.gluon import nn, rnn\r\n",
      "import os\r\n",
      "from lstnet import LSTNet\r\n",
      "from timeseriesdataset import TimeSeriesData, TimeSeriesDataset\r\n",
      "import re\r\n",
      "import time\r\n",
      "\r\n",
      "def get_first_file_path_in_dir(input_dir):\r\n",
      "    for root, dirs, files in os.walk(input_dir):\r\n",
      "        for filename in files:\r\n",
      "            return os.path.join(input_dir,filename)\r\n",
      "    return None\r\n",
      "\r\n",
      "def load_file(file_path,hyperparameters):\r\n",
      "    print('Loading file {}'.format(file_path))\r\n",
      "    if not file_path:\r\n",
      "        print('Could not load data file: ' + file_path)\r\n",
      "        return None\r\n",
      "    ts_data = TimeSeriesData(file_path,\r\n",
      "        window=hyperparameters['window'],\r\n",
      "        horizon=hyperparameters['horizon'],\r\n",
      "        train_ratio=1.0)\r\n",
      "    return ts_data\r\n",
      "\r\n",
      "def train(\r\n",
      "    hyperparameters,\r\n",
      "    input_data_config,\r\n",
      "    channel_input_dirs,\r\n",
      "    output_data_dir,\r\n",
      "    model_dir,\r\n",
      "    num_gpus,\r\n",
      "    num_cpus,\r\n",
      "    hosts,\r\n",
      "    current_host,\r\n",
      "    **kwargs):\r\n",
      "\r\n",
      "    \"\"\"\r\n",
      "    [Required]\r\n",
      "\r\n",
      "    Runs Apache MXNet training. Amazon SageMaker calls this function with information\r\n",
      "    about the training environment. When called, if this function returns an\r\n",
      "    object, that object is passed to a save function.  The save function\r\n",
      "    can be used to serialize the model to the Amazon SageMaker training job model\r\n",
      "    directory.\r\n",
      "\r\n",
      "    The **kwargs parameter can be used to absorb any Amazon SageMaker parameters that\r\n",
      "    your training job doesn't need to use. For example, if your training job\r\n",
      "    doesn't need to know anything about the training environment, your function\r\n",
      "    signature can be as simple as train(**kwargs).\r\n",
      "\r\n",
      "    Amazon SageMaker invokes your train function with the following python kwargs:\r\n",
      "\r\n",
      "    Args:\r\n",
      "        - hyperparameters: The Amazon SageMaker Hyperparameters dictionary. A dict\r\n",
      "            of string to string.\r\n",
      "        - input_data_config: The Amazon SageMaker input channel configuration for\r\n",
      "            this job.\r\n",
      "        - channel_input_dirs: A dict of string-to-string maps from the\r\n",
      "            Amazon SageMaker algorithm input channel name to the directory containing\r\n",
      "            files for that input channel. Note, if the Amazon SageMaker training job\r\n",
      "            is run in PIPE mode, this dictionary will be empty.\r\n",
      "        - output_data_dir:\r\n",
      "            The Amazon SageMaker output data directory. After the function returns, data written to this\r\n",
      "            directory is made available in the Amazon SageMaker training job\r\n",
      "            output location.\r\n",
      "        - model_dir: The Amazon SageMaker model directory. After the function returns, data written to this\r\n",
      "            directory is made available to the Amazon SageMaker training job\r\n",
      "            model location.\r\n",
      "        - num_gpus: The number of GPU devices available on the host this script\r\n",
      "            is being executed on.\r\n",
      "        - num_cpus: The number of CPU devices available on the host this script\r\n",
      "            is being executed on.\r\n",
      "        - hosts: A list of hostnames in the Amazon SageMaker training job cluster.\r\n",
      "        - current_host: This host's name. It will exist in the hosts list.\r\n",
      "        - kwargs: Other keyword args.\r\n",
      "\r\n",
      "    Returns:\r\n",
      "        - (object): Optional. An Apache MXNet model to be passed to the model\r\n",
      "            save function. If you do not return anything (or return None),\r\n",
      "            the save function is not called.\r\n",
      "    \"\"\"\r\n",
      "\r\n",
      "    train_file_path = get_first_file_path_in_dir(channel_input_dirs['train'])\r\n",
      "    print('Train file path {}'.format(train_file_path))\r\n",
      "    test_file_path = get_first_file_path_in_dir(channel_input_dirs['test'])\r\n",
      "    print('Test file path {}'.format(test_file_path))\r\n",
      "    ts_data_train = load_file(train_file_path,hyperparameters)\r\n",
      "    ts_data_test = load_file(test_file_path,hyperparameters)\r\n",
      "\r\n",
      "    ctx = mx.gpu() if num_gpus > 0 else mx.cpu()\r\n",
      "    print('Running on {}'.format(ctx))\r\n",
      "    print('Hosts {}'.format(hosts))\r\n",
      "    print('Current Host {}'.format(current_host))\r\n",
      "\r\n",
      "    net = LSTNet(\r\n",
      "        num_series=ts_data_train.num_series,\r\n",
      "        conv_hid=hyperparameters['conv_hid'],\r\n",
      "        gru_hid=hyperparameters['gru_hid'],\r\n",
      "        skip_gru_hid=hyperparameters['skip_gru_hid'],\r\n",
      "        skip=hyperparameters['skip'],\r\n",
      "        ar_window=hyperparameters['ar_window'])\r\n",
      "\r\n",
      "    net.initialize(init=mx.init.Xavier(factor_type=\"in\", magnitude=2.34), ctx=ctx)\r\n",
      "\r\n",
      "    kvstore = 'local'\r\n",
      "    if len(hosts) == 1:\r\n",
      "        kvstore = 'device' if num_gpus > 0 else 'local'\r\n",
      "    else:\r\n",
      "        kvstore = 'dist_device_sync' if num_gpus > 0 else 'dist_sync'\r\n",
      "    print('kvstore {}'.format(kvstore))\r\n",
      "    store = kv.create(kvstore)\r\n",
      "    trainer = gluon.Trainer(net.collect_params(),\r\n",
      "        kvstore=store,\r\n",
      "        optimizer='adam',\r\n",
      "        optimizer_params={'learning_rate': hyperparameters['learning_rate'], 'clip_gradient': hyperparameters['clip_gradient']})\r\n",
      "\r\n",
      "    batch_size = hyperparameters['batch_size']\r\n",
      "    train_data_loader = gluon.data.DataLoader(\r\n",
      "        ts_data_train.train, batch_size=batch_size, shuffle=True, num_workers=16, last_batch='discard')\r\n",
      "    test_data_loader = gluon.data.DataLoader(\r\n",
      "        ts_data_test.train, batch_size=batch_size, shuffle=True, num_workers=16, last_batch='discard')\r\n",
      "\r\n",
      "    epochs = hyperparameters['epochs']\r\n",
      "    print(\"Training Start\")\r\n",
      "    metric = mx.metric.RMSE()\r\n",
      "    tic = time.time()\r\n",
      "    for e in range(epochs):\r\n",
      "        metric.reset()\r\n",
      "        epoch_start_time = time.time()\r\n",
      "        for data, label in train_data_loader:\r\n",
      "            batch_forward_backward(data,label,ctx,net,trainer,batch_size,metric)\r\n",
      "        name, value = metric.get()\r\n",
      "        print(\"Epoch {}: {} {} time {:.4f} s\".format(e, name, value, time.time()-epoch_start_time))\r\n",
      "\r\n",
      "    # Calculate the test RMSE when training has finished\r\n",
      "    validate(train_data_loader,metric,ctx,net)\r\n",
      "\r\n",
      "    print(\"Total training time: {}\".format(time.time()-tic))\r\n",
      "\r\n",
      "    if not os.path.exists(output_data_dir):\r\n",
      "        os.makedirs(output_data_dir)\r\n",
      "    net.save_params(os.path.join(output_data_dir,'lstnet_params.params'))\r\n",
      "    print(\"Training End\")\r\n",
      "    return\r\n",
      "\r\n",
      "def validate(data_loader,metric,ctx,net):\r\n",
      "    metric.reset()\r\n",
      "    for data, label in data_loader:\r\n",
      "        data = data.as_in_context(ctx)\r\n",
      "        label = label.as_in_context(ctx)\r\n",
      "        metric.update(label,net(data))\r\n",
      "    name, value = metric.get()\r\n",
      "    print('Final {} {}'.format(name,value))\r\n",
      "    return name,value\r\n",
      "\r\n",
      "def batch_forward_backward(data, label, ctx, net, trainer, batch_size, metric):\r\n",
      "    l1 = gluon.loss.L1Loss()\r\n",
      "    data = data.as_in_context(ctx)\r\n",
      "    label = label.as_in_context(ctx)\r\n",
      "    with autograd.record():\r\n",
      "        z = net(data)\r\n",
      "        loss = l1(z,label)\r\n",
      "    autograd.backward(loss)\r\n",
      "    trainer.step(batch_size)\r\n",
      "    metric.update(label,z)\r\n"
     ]
    }
   ],
   "source": [
    "!cat lstnet_sagemaker.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Locally\n",
    "\n",
    "To make sure there are no obvious bugs in the code, call the `train()` function. This is done with 1 epoch to verify that it executed correctly. There are also some basic unit tests included in the module.\n",
    "\n",
    "The key parameters to the `train()` function in this case are:\n",
    "\n",
    "* `hyperparameters`: The Amazon SageMaker Hyperparameters dictionary. A dictionary of string-to-string.\n",
    "* `channel_input_dirs`: A dictionary of string-to-string maps from the Amazon SageMaker algorithm input channel name to the directory containing files for that input channel.\n",
    "  * **Note:** If the Amazon SageMaker training job is run in PIPE mode, this dictionary will be empty.\n",
    "* `output_data_dir`: The Amazon SageMaker output data directory. After the function returns, data written to this directory is made available in the Amazon SageMaker training job output location.\n",
    "* `num_gpus`: The number of GPU devices available on the host this script is being executed on. As we are running on a CPU notebook instance, this number is set to zero.\n",
    "* `num_cpus`: The number of CPU devices available on the host this script is being executed on.\n",
    "* `hosts`: A list of hostnames in the Amazon SageMaker training job cluster.\n",
    "* `current_host`: This host's name. It will exist in the hosts list.\n",
    "* `kwargs`: Other keyword agruments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train file path ./data/test.csv\n",
      "Test file path ./data/test.csv\n",
      "Loading file ./data/test.csv\n",
      "Loading file ./data/test.csv\n",
      "Is it a file True\n",
      "Data length 5261\n",
      "Loading file ./data/test.csv\n",
      "Loading file ./data/test.csv\n",
      "Is it a file True\n",
      "Data length 5261\n",
      "Running on cpu(0)\n",
      "Hosts ['localhost']\n",
      "Current Host localhost\n",
      "kvstore local\n",
      "Training Start\n",
      "Epoch 0: rmse 0.40146579841772717 time 51.6454 s\n",
      "Final rmse 0.2545755910567748\n",
      "Total training time: 79.86906147003174\n",
      "Training End\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages/mxnet/gluon/block.py:345: UserWarning: save_params is deprecated. Please use save_parameters. Note that if you want load from SymbolBlock later, please use export instead. For details, see https://mxnet.incubator.apache.org/tutorials/gluon/save_load_params.html\n",
      "  warnings.warn(\"save_params is deprecated. Please use save_parameters. \"\n"
     ]
    }
   ],
   "source": [
    "from lstnet_sagemaker import train\n",
    "hyperparameters = {\n",
    "    'conv_hid' : 10,\n",
    "    'gru_hid' : 10,\n",
    "    'skip_gru_hid' : 2,\n",
    "    'skip' : 5,\n",
    "    'ar_window' : 6,\n",
    "    'window' : 24*7,\n",
    "    'horizon' : 24,\n",
    "    'learning_rate' : 0.01,\n",
    "    'clip_gradient' : 10.,\n",
    "    'batch_size' : 128,\n",
    "    'epochs' : 1\n",
    "}\n",
    "channel_input_dirs = {'train':data_dir,'test':data_dir}\n",
    "train(\n",
    "    hyperparameters = hyperparameters,\n",
    "    input_data_config = None,\n",
    "    channel_input_dirs = channel_input_dirs,\n",
    "    output_data_dir = os.path.join(data_dir,'output'),\n",
    "    model_dir = None,\n",
    "    num_gpus = 0,\n",
    "    num_cpus = 1,\n",
    "    hosts = ['localhost'],\n",
    "    current_host = 'localhost')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose Hyperparameters\n",
    "\n",
    "One parameter to watch when switching to multi-GPU is `batch_size`. When using multiple GPUs, each batch is split across all available GPUs. To improve performance, it is common to increase the batch size. Increasing the number of epochs may increase accuracy but will also increase training time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    'conv_hid' : 100,\n",
    "    'gru_hid' : 100,\n",
    "    'skip_gru_hid' : 5,\n",
    "    'skip' : 24,\n",
    "    'ar_window' : 24,\n",
    "    'window' : 24*7,\n",
    "    'horizon' : 24,\n",
    "    'learning_rate' : 0.0001,\n",
    "    'clip_gradient' : 10.,\n",
    "    'batch_size' : 512,\n",
    "    'epochs' : 10\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trigger the Training Job Using the Amazon SageMaker Python SDK\n",
    "\n",
    "The main differences to moving from a single host and GPU, include:\n",
    "  * The `train_instance_count` is set to 2 to run on a 2-node CPU cluster.\n",
    "  * The `train_instance_type` of `ml.p3.2xlarge` is chosen with a single GPU. The code is designed so that you could take this and run it on multi-GPU instances by only changing the instance type.\n",
    "\n",
    "Amazon SageMaker automatically bootstraps an MXNet cluster with 2 nodes.\n",
    "\n",
    "First an estimator is created with `sagemaker.mxnet.MXNet`. The inputs are:\n",
    "  * `entry_point='lstnet_sagemaker.py'`: The module used to run the training by calling the `train()` function.\n",
    "  * `source_dir='.'`: An optional directory containing code is copied onto the Amazon SageMaker training hosts and made available to the training script.\n",
    "  * `role=role`: The IAM role which is given to the training hosts giving them privileges such as access to the Amazon S3 bucket.\n",
    "  * `output_path='s3://{}/{}/output'.format(bucket, prefix)`: The Amazon S3 bucket to store artifacts such as the model parameters.\n",
    "  * `train_instance_count=2`: The number of hosts used for training.\n",
    "    * Using a number > 1 will start a cluster.\n",
    "    * To take advantage of this, the training data is sharded.\n",
    "  * `train_instance_type='ml.p3.2xlarge'`: The Amazon EC2 instance type to be used for training hosts.\n",
    "    * In this case, choose the latest generation `p3` instance type, each of which contains 1 Nvidia Tesla v100 GPU.\n",
    "  * `hyperparameters=hyperparameters`: The hyperparameter dictionary made available to the `train()` function in the endpoint script.\n",
    "\n",
    "Then, the `fit()` method of the estimator is called. The parameters are:\n",
    "\n",
    "  * `inputs`: A dictionary containing the URLs in Amazon S3 of the `train/` data directory and the `test/` data directory.\n",
    "  * `wait`: This is specified as `False` so the `fit()` method returns immediately after the training job is created.\n",
    "    * Go to the Amazon SageMaker console to monitor the progress of the job.\n",
    "    * Set `wait` to `True` to block and see the progress of the training job output in the notebook. If you leave it as `False` the training jobs will run in the background and allow you to continue with the notebook.\n",
    "\n",
    "You will run two different versions to compare training speeds:\n",
    "\n",
    "1. Two hosts each with 4 CPUs (`ml.m4.xlarge`)\n",
    "1. One host with 1 GPU (`ml.p3.2xlarge`)\n",
    "\n",
    "**If you receive an error with 'resource limit exceeded' try reducing the number of instances or changing the instance type from a GPU to a CPU instance**\n",
    "\n",
    "**Challenge:** Experiment with more combinations to improve performance. Can you find the most efficient batch-size vs hardware combination for this network? Which job is faster - a single GPU or multiple CPUs? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The Python 2 mxnet images will be soon deprecated and may not be supported for newer upcoming versions of the mxnet images.\n",
      "Please set the argument \"py_version='py3'\" to use the Python 3 mxnet image.\n"
     ]
    }
   ],
   "source": [
    "lstnet1 = MXNet(entry_point='lstnet_sagemaker.py',\n",
    "                source_dir='.',\n",
    "                role=role,\n",
    "                output_path='s3://{}/{}/output'.format(bucket, prefix),\n",
    "                train_instance_count=2,\n",
    "                train_instance_type='ml.m4.xlarge',\n",
    "                hyperparameters=hyperparameters,\n",
    "                framework_version=1.2)\n",
    "lstnet1.fit(inputs={\n",
    "                'train': 's3://{}/{}{}'.format(bucket, prefix, multiple_host_train_bucket_prefix),\n",
    "                'test': 's3://{}/{}{}'.format(bucket, prefix, test_bucket_prefix)\n",
    "            },\n",
    "            wait=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The Python 2 mxnet images will be soon deprecated and may not be supported for newer upcoming versions of the mxnet images.\n",
      "Please set the argument \"py_version='py3'\" to use the Python 3 mxnet image.\n"
     ]
    }
   ],
   "source": [
    "lstnet3 = MXNet(entry_point='lstnet_sagemaker.py',\n",
    "    source_dir='.',\n",
    "    role=role,\n",
    "    output_path='s3://{}/{}/output'.format(bucket, prefix),\n",
    "    train_instance_count=1,\n",
    "    train_instance_type='ml.p3.2xlarge',\n",
    "    hyperparameters=hyperparameters, \n",
    "    framework_version=1.2)\n",
    "lstnet3.fit(inputs={\n",
    "                'train': 's3://{}/{}{}'.format(bucket, prefix, single_host_train_bucket_prefix),\n",
    "                'test': 's3://{}/{}{}'.format(bucket, prefix, test_bucket_prefix)\n",
    "            },\n",
    "            wait=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "\n",
    "## Section 3: Challenge - Deploying an Endpoint Using Amazon SageMaker\n",
    "<a name=\"section3\"></a>\n",
    "\n",
    "Amazon SageMaker is composed of 5 main services:\n",
    "  * Hosted Notebooks\n",
    "  * Built-in Algorithms\n",
    "  * Model Training\n",
    "  * Hyperparameter Tuning\n",
    "  * Model Hosting\n",
    "\n",
    "Each service works together, and can also be used independendly as needed. In this section, you will learn how to deploy and host a model so it can be used in production to evaluate novel inputs. The hosted endpoints can either be used on single examples or in batch mode over a larger number of examples.\n",
    "\n",
    "In this challenge portion of the lab you have the option to explore three different ways of deploying the endpoint. You can either choose the option that you feel most comfortable with or challenge yourself with an option you may be less familiar with. If there is time remaining, feel free to work through all three options.\n",
    "\n",
    "1. <a href=\"#s3p1\">Using a model trained with Amazon SageMaker. (Easy)</a>  \n",
    "1. <a href=\"#s3p2\">From a model trained elsewhere with artifacts stored in Amazon S3. (Moderate)</a> \n",
    "1. <a href=\"#s3p3\">From a model developed with a custom Docker container. (Harder)</a>\n",
    "<div style=\"text-align: right\"><a href=\"#toc\">Back to top</a></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_session = sagemaker.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"s3p1\"></a>\n",
    "### Challenge Option 1: Deploying a Model Trained with Amazon SageMaker\n",
    "\n",
    "\n",
    "A model is trained using Amazon SageMaker and then directly deployed to an endpoint.\n",
    "\n",
    "MNIST is a widely used dataset for handwritten digit classification. It consists of 70,000 labeled 28x28 pixel grayscale images of hand-written digits. The dataset is split into 60,000 training images and 10,000 test images. There are 10 classes (one for each of the 10 digits). In this part of the lab, you will train and test an MNIST model on Amazon SageMaker using MXNet and the Gluon API.\n",
    "\n",
    "#### Download Training and Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<mxnet.gluon.data.vision.datasets.MNIST at 0x7fe02cb60390>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gluon.data.vision.MNIST('./data/train', train=True)\n",
    "gluon.data.vision.MNIST('./data/test', train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Uploading the Data\n",
    "\n",
    "Use the `sagemaker.Session.upload_data` function to upload your datasets to an Amazon S3 location. The return value `inputs` identifies the location. You will use this later when starting the training job.\n",
    "\n",
    "**Note:** Save the bucket name that is included in the output of the below code cell. It will look similar to `sagemaker-REGION-ACCOUNT_ID`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://sagemaker-us-west-2-569442446084/data/DEMO-mnist'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = sagemaker_session.upload_data(path='data', key_prefix='data/DEMO-mnist')\n",
    "inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implement the Training Function\n",
    "\n",
    "You will need to provide a training script that can run on the Amazon SageMaker platform. The training scripts are the same as the ones you would write for local training, except you need to provide a `train` function. When Amazon SageMaker calls your function, it will pass in arguments that describe the training environment. Check the script below to see how this works.\n",
    "\n",
    "The script here is an adaptation of the [Gluon MNIST example](https://github.com/gluon-api/gluon-api/blob/master/tutorials/mnist-gluon-example.ipynb) provided by the [Apache MXNet](https://mxnet.incubator.apache.org/) project. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from __future__ import print_function\r\n",
      "\r\n",
      "import logging\r\n",
      "import mxnet as mx\r\n",
      "from mxnet import gluon, autograd\r\n",
      "from mxnet.gluon import nn\r\n",
      "import numpy as np\r\n",
      "import json\r\n",
      "import time\r\n",
      "\r\n",
      "\r\n",
      "logging.basicConfig(level=logging.DEBUG)\r\n",
      "\r\n",
      "# ------------------------------------------------------------ #\r\n",
      "# Training methods                                             #\r\n",
      "# ------------------------------------------------------------ #\r\n",
      "\r\n",
      "\r\n",
      "def train(current_host, channel_input_dirs, hyperparameters, hosts, num_gpus):\r\n",
      "    # SageMaker passes num_cpus, num_gpus and other args we can use to tailor training to\r\n",
      "    # the current container environment, but here we just use simple cpu context.\r\n",
      "    ctx = mx.cpu()\r\n",
      "\r\n",
      "    # retrieve the hyperparameters we set in notebook (with some defaults)\r\n",
      "    batch_size = hyperparameters.get('batch_size', 100)\r\n",
      "    epochs = hyperparameters.get('epochs', 10)\r\n",
      "    learning_rate = hyperparameters.get('learning_rate', 0.1)\r\n",
      "    momentum = hyperparameters.get('momentum', 0.9)\r\n",
      "    log_interval = hyperparameters.get('log_interval', 100)\r\n",
      "\r\n",
      "    # load training and validation data\r\n",
      "    # we use the gluon.data.vision.MNIST class because of its built in mnist pre-processing logic,\r\n",
      "    # but point it at the location where SageMaker placed the data files, so it doesn't download them again.\r\n",
      "    training_dir = channel_input_dirs['training']\r\n",
      "    train_data = get_train_data(training_dir + '/train', batch_size)\r\n",
      "    val_data = get_val_data(training_dir + '/test', batch_size)\r\n",
      "\r\n",
      "    # define the network\r\n",
      "    net = define_network()\r\n",
      "\r\n",
      "    # Collect all parameters from net and its children, then initialize them.\r\n",
      "    net.initialize(mx.init.Xavier(magnitude=2.24), ctx=ctx)\r\n",
      "    # Trainer is for updating parameters with gradient.\r\n",
      "\r\n",
      "    if len(hosts) == 1:\r\n",
      "        kvstore = 'device' if num_gpus > 0 else 'local'\r\n",
      "    else:\r\n",
      "        kvstore = 'dist_device_sync' if num_gpus > 0 else 'dist_sync'\r\n",
      "\r\n",
      "    trainer = gluon.Trainer(net.collect_params(), 'sgd',\r\n",
      "                            {'learning_rate': learning_rate, 'momentum': momentum},\r\n",
      "                            kvstore=kvstore)\r\n",
      "    metric = mx.metric.Accuracy()\r\n",
      "    loss = gluon.loss.SoftmaxCrossEntropyLoss()\r\n",
      "\r\n",
      "    # shard the training data in case we are doing distributed training. Alternatively to splitting in memory,\r\n",
      "    # the data could be pre-split in S3 and use ShardedByS3Key to do distributed training.\r\n",
      "    if len(hosts) > 1:\r\n",
      "        train_data = [x for x in train_data]\r\n",
      "        shard_size = len(train_data) // len(hosts)\r\n",
      "        for i, host in enumerate(hosts):\r\n",
      "            if host == current_host:\r\n",
      "                start = shard_size * i\r\n",
      "                end = start + shard_size\r\n",
      "                break\r\n",
      "\r\n",
      "        train_data = train_data[start:end]\r\n",
      "\r\n",
      "    for epoch in range(epochs):\r\n",
      "        # reset data iterator and metric at begining of epoch.\r\n",
      "        metric.reset()\r\n",
      "        btic = time.time()\r\n",
      "        for i, (data, label) in enumerate(train_data):\r\n",
      "            # Copy data to ctx if necessary\r\n",
      "            data = data.as_in_context(ctx)\r\n",
      "            label = label.as_in_context(ctx)\r\n",
      "            # Start recording computation graph with record() section.\r\n",
      "            # Recorded graphs can then be differentiated with backward.\r\n",
      "            with autograd.record():\r\n",
      "                output = net(data)\r\n",
      "                L = loss(output, label)\r\n",
      "                L.backward()\r\n",
      "            # take a gradient step with batch_size equal to data.shape[0]\r\n",
      "            trainer.step(data.shape[0])\r\n",
      "            # update metric at last.\r\n",
      "            metric.update([label], [output])\r\n",
      "\r\n",
      "            if i % log_interval == 0 and i > 0:\r\n",
      "                name, acc = metric.get()\r\n",
      "                print('[Epoch %d Batch %d] Training: %s=%f, %f samples/s' %\r\n",
      "                      (epoch, i, name, acc, batch_size / (time.time() - btic)))\r\n",
      "\r\n",
      "            btic = time.time()\r\n",
      "\r\n",
      "        name, acc = metric.get()\r\n",
      "        print('[Epoch %d] Training: %s=%f' % (epoch, name, acc))\r\n",
      "\r\n",
      "        name, val_acc = test(ctx, net, val_data)\r\n",
      "        print('[Epoch %d] Validation: %s=%f' % (epoch, name, val_acc))\r\n",
      "\r\n",
      "    return net\r\n",
      "\r\n",
      "\r\n",
      "def save(net, model_dir):\r\n",
      "    # save the model\r\n",
      "    y = net(mx.sym.var('data'))\r\n",
      "    y.save('%s/model.json' % model_dir)\r\n",
      "    net.collect_params().save('%s/model.params' % model_dir)\r\n",
      "\r\n",
      "\r\n",
      "def define_network():\r\n",
      "    net = nn.Sequential()\r\n",
      "    with net.name_scope():\r\n",
      "        net.add(nn.Dense(128, activation='relu'))\r\n",
      "        net.add(nn.Dense(64, activation='relu'))\r\n",
      "        net.add(nn.Dense(10))\r\n",
      "    return net\r\n",
      "\r\n",
      "\r\n",
      "def input_transformer(data, label):\r\n",
      "    data = data.reshape((-1,)).astype(np.float32) / 255\r\n",
      "    return data, label\r\n",
      "\r\n",
      "\r\n",
      "def get_train_data(data_dir, batch_size):\r\n",
      "    return gluon.data.DataLoader(\r\n",
      "        gluon.data.vision.MNIST(data_dir, train=True, transform=input_transformer),\r\n",
      "        batch_size=batch_size, shuffle=True, last_batch='discard')\r\n",
      "\r\n",
      "\r\n",
      "def get_val_data(data_dir, batch_size):\r\n",
      "    return gluon.data.DataLoader(\r\n",
      "        gluon.data.vision.MNIST(data_dir, train=False, transform=input_transformer),\r\n",
      "        batch_size=batch_size, shuffle=False)\r\n",
      "\r\n",
      "\r\n",
      "def test(ctx, net, val_data):\r\n",
      "    metric = mx.metric.Accuracy()\r\n",
      "    for data, label in val_data:\r\n",
      "        data = data.as_in_context(ctx)\r\n",
      "        label = label.as_in_context(ctx)\r\n",
      "        output = net(data)\r\n",
      "        metric.update([label], [output])\r\n",
      "    return metric.get()\r\n",
      "\r\n",
      "\r\n",
      "# ------------------------------------------------------------ #\r\n",
      "# Hosting methods                                              #\r\n",
      "# ------------------------------------------------------------ #\r\n",
      "\r\n",
      "def model_fn(model_dir):\r\n",
      "    \"\"\"\r\n",
      "    Load the gluon model. Called once when hosting service starts.\r\n",
      "\r\n",
      "    :param: model_dir The directory where model files are stored.\r\n",
      "    :return: a model (in this case a Gluon network)\r\n",
      "    \"\"\"\r\n",
      "    symbol = mx.sym.load('%s/model.json' % model_dir)\r\n",
      "    outputs = mx.symbol.softmax(data=symbol, name='softmax_label')\r\n",
      "    inputs = mx.sym.var('data')\r\n",
      "    param_dict = gluon.ParameterDict('model_')\r\n",
      "    net = gluon.SymbolBlock(outputs, inputs, param_dict)\r\n",
      "    net.load_params('%s/model.params' % model_dir, ctx=mx.cpu())\r\n",
      "    return net\r\n",
      "\r\n",
      "\r\n",
      "def transform_fn(net, data, input_content_type, output_content_type):\r\n",
      "    \"\"\"\r\n",
      "    Transform a request using the Gluon model. Called once per request.\r\n",
      "\r\n",
      "    :param net: The Gluon model.\r\n",
      "    :param data: The request payload.\r\n",
      "    :param input_content_type: The request content type.\r\n",
      "    :param output_content_type: The (desired) response content type.\r\n",
      "    :return: response payload and content type.\r\n",
      "    \"\"\"\r\n",
      "    # we can use content types to vary input/output handling, but\r\n",
      "    # here we just assume json for both\r\n",
      "    parsed = json.loads(data)\r\n",
      "    nda = mx.nd.array(parsed)\r\n",
      "    output = net(nda)\r\n",
      "    prediction = mx.nd.argmax(output, axis=1)\r\n",
      "    response_body = json.dumps(prediction.asnumpy().tolist()[0])\r\n",
      "    return response_body, output_content_type\r\n"
     ]
    }
   ],
   "source": [
    "!cat mnist.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run the Training Script on Amazon SageMaker\n",
    "\n",
    "The `MXNet` class allows you to run your training function on Amazon SageMaker infrastructure. You need to configure it with your training script, an IAM role, the number of training instances, and the training instance type. In this case, you will run your training job on a single `c4.xlarge` instance.\n",
    "\n",
    "After you have constructed your `MXNet` object, fit it using the data uploaded to Amazon S3. Amazon SageMaker makes sure your data is available in the local filesystem, so your training script can simply read the data from disk.\n",
    "\n",
    "**Note:** You can ignore any warnings outputted by the below code cell. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The Python 2 mxnet images will be soon deprecated and may not be supported for newer upcoming versions of the mxnet images.\n",
      "Please set the argument \"py_version='py3'\" to use the Python 3 mxnet image.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-30 03:18:07 Starting - Starting the training job...\n",
      "2019-05-30 03:18:09 Starting - Launching requested ML instances......\n",
      "2019-05-30 03:19:17 Starting - Preparing the instances for training...\n",
      "2019-05-30 03:20:01 Downloading - Downloading input data...\n",
      "2019-05-30 03:20:32 Training - Training image download completed. Training in progress..\n",
      "\u001b[31m2019-05-30 03:20:32,047 INFO - root - running container entrypoint\u001b[0m\n",
      "\u001b[31m2019-05-30 03:20:32,047 INFO - root - starting train task\u001b[0m\n",
      "\u001b[31m2019-05-30 03:20:32,052 INFO - container_support.training - Training starting\u001b[0m\n",
      "\u001b[31m2019-05-30 03:20:33,086 WARNING - mxnet_container.train - #033[1;33mThis required structure for training scripts will be deprecated with the next major release of MXNet images. The train() function will no longer be required; instead the training script must be able to be run as a standalone script. For more information, see https://github.com/aws/sagemaker-python-sdk/tree/master/src/sagemaker/mxnet#updating-your-mxnet-training-script.#033[1;0m\u001b[0m\n",
      "\u001b[31m2019-05-30 03:20:34,499 INFO - mxnet_container.train - MXNetTrainingEnvironment: {'enable_cloudwatch_metrics': False, 'available_gpus': 0, 'channels': {u'training': {u'TrainingInputMode': u'File', u'RecordWrapperType': u'None', u'S3DistributionType': u'FullyReplicated'}}, '_ps_verbose': 0, 'resource_config': {u'hosts': [u'algo-1'], u'network_interface_name': u'ethwe', u'current_host': u'algo-1'}, 'user_script_name': u'mnist.py', 'input_config_dir': '/opt/ml/input/config', 'channel_dirs': {u'training': u'/opt/ml/input/data/training'}, 'code_dir': '/opt/ml/code', 'output_data_dir': '/opt/ml/output/data/', 'output_dir': '/opt/ml/output', 'model_dir': '/opt/ml/model', 'hyperparameters': {u'sagemaker_program': u'mnist.py', u'learning_rate': 0.1, u'batch_size': 100, u'epochs': 10, u'log_interval': 100, u'sagemaker_region': u'us-west-2', u'sagemaker_enable_cloudwatch_metrics': False, u'sagemaker_job_name': u'sagemaker-mxnet-2019-05-30-03-18-06-821', u'sagemaker_container_log_level': 20, u'momentum': 0.9, u'sagemaker_submit_directory': u's3://sagemaker-us-west-2-569442446084/sagemaker-mxnet-2019-05-30-03-18-06-821/source/sourcedir.tar.gz'}, 'hosts': [u'algo-1'], 'job_name': 'sagemaker-mxnet-2019-05-30-03-18-06-821', '_ps_port': 8000, 'user_script_archive': u's3://sagemaker-us-west-2-569442446084/sagemaker-mxnet-2019-05-30-03-18-06-821/source/sourcedir.tar.gz', '_scheduler_host': u'algo-1', 'sagemaker_region': u'us-west-2', '_scheduler_ip': '10.32.0.3', 'input_dir': '/opt/ml/input', 'user_requirements_file': None, 'current_host': u'algo-1', 'container_log_level': 20, 'available_cpus': 4, 'base_dir': '/opt/ml'}\u001b[0m\n",
      "\u001b[31mDownloading s3://sagemaker-us-west-2-569442446084/sagemaker-mxnet-2019-05-30-03-18-06-821/source/sourcedir.tar.gz to /tmp/script.tar.gz\u001b[0m\n",
      "\u001b[31m2019-05-30 03:20:34,795 INFO - mxnet_container.train - Starting distributed training task\u001b[0m\n",
      "\u001b[31m[Epoch 0 Batch 100] Training: accuracy=0.799109, 7467.692198 samples/s\u001b[0m\n",
      "\u001b[31m[Epoch 0 Batch 200] Training: accuracy=0.857065, 5842.462739 samples/s\u001b[0m\n",
      "\u001b[31m[Epoch 0 Batch 300] Training: accuracy=0.883754, 7732.148585 samples/s\u001b[0m\n",
      "\u001b[31m[Epoch 0 Batch 400] Training: accuracy=0.899925, 5960.951068 samples/s\u001b[0m\n",
      "\u001b[31m[Epoch 0 Batch 500] Training: accuracy=0.909880, 7519.908204 samples/s\u001b[0m\n",
      "\u001b[31m[Epoch 0] Training: accuracy=0.917067\u001b[0m\n",
      "\u001b[31m[Epoch 0] Validation: accuracy=0.957900\u001b[0m\n",
      "\u001b[31m[Epoch 1 Batch 100] Training: accuracy=0.962871, 5975.472988 samples/s\u001b[0m\n",
      "\u001b[31m[Epoch 1 Batch 200] Training: accuracy=0.964179, 7774.284073 samples/s\u001b[0m\n",
      "\u001b[31m[Epoch 1 Batch 300] Training: accuracy=0.964850, 6018.948124 samples/s\u001b[0m\n",
      "\u001b[31m[Epoch 1 Batch 400] Training: accuracy=0.963591, 7676.392321 samples/s\u001b[0m\n",
      "\u001b[31m[Epoch 1 Batch 500] Training: accuracy=0.963752, 6788.327642 samples/s\u001b[0m\n",
      "\u001b[31m[Epoch 1] Training: accuracy=0.963650\u001b[0m\n",
      "\u001b[31m[Epoch 1] Validation: accuracy=0.966200\u001b[0m\n",
      "\u001b[31m[Epoch 2 Batch 100] Training: accuracy=0.976040, 7043.213380 samples/s\u001b[0m\n",
      "\u001b[31m[Epoch 2 Batch 200] Training: accuracy=0.976269, 5876.432925 samples/s\u001b[0m\n",
      "\u001b[31m[Epoch 2 Batch 300] Training: accuracy=0.975249, 5804.863331 samples/s\u001b[0m\n",
      "\u001b[31m[Epoch 2 Batch 400] Training: accuracy=0.974638, 5943.213410 samples/s\u001b[0m\n",
      "\u001b[31m[Epoch 2 Batch 500] Training: accuracy=0.974172, 5845.556918 samples/s\u001b[0m\n",
      "\u001b[31m[Epoch 2] Training: accuracy=0.973950\u001b[0m\n",
      "\u001b[31m[Epoch 2] Validation: accuracy=0.967100\u001b[0m\n",
      "\u001b[31m[Epoch 3 Batch 100] Training: accuracy=0.981881, 7133.170068 samples/s\u001b[0m\n",
      "\u001b[31m[Epoch 3 Batch 200] Training: accuracy=0.981194, 5779.029458 samples/s\u001b[0m\n",
      "\u001b[31m[Epoch 3 Batch 300] Training: accuracy=0.979435, 7649.372629 samples/s\u001b[0m\n",
      "\u001b[31m[Epoch 3 Batch 400] Training: accuracy=0.979277, 7644.075087 samples/s\u001b[0m\n",
      "\u001b[31m[Epoch 3 Batch 500] Training: accuracy=0.978723, 7517.751649 samples/s\u001b[0m\n",
      "\u001b[31m[Epoch 3] Training: accuracy=0.978417\u001b[0m\n",
      "\u001b[31m[Epoch 3] Validation: accuracy=0.970100\u001b[0m\n",
      "\u001b[31m[Epoch 4 Batch 100] Training: accuracy=0.982277, 5873.388226 samples/s\u001b[0m\n",
      "\u001b[31m[Epoch 4 Batch 200] Training: accuracy=0.982886, 6161.841661 samples/s\u001b[0m\n",
      "\u001b[31m[Epoch 4 Batch 300] Training: accuracy=0.982126, 6267.264360 samples/s\u001b[0m\n",
      "\u001b[31m[Epoch 4 Batch 400] Training: accuracy=0.982618, 7660.549386 samples/s\u001b[0m\n",
      "\u001b[31m[Epoch 4 Batch 500] Training: accuracy=0.982435, 7573.406522 samples/s\u001b[0m\n",
      "\u001b[31m[Epoch 4] Training: accuracy=0.982250\u001b[0m\n",
      "\u001b[31m[Epoch 4] Validation: accuracy=0.977600\u001b[0m\n",
      "\u001b[31m[Epoch 5 Batch 100] Training: accuracy=0.987624, 5691.899741 samples/s\u001b[0m\n",
      "\u001b[31m[Epoch 5 Batch 200] Training: accuracy=0.987313, 5658.649254 samples/s\u001b[0m\n",
      "\u001b[31m[Epoch 5 Batch 300] Training: accuracy=0.986113, 7405.197740 samples/s\u001b[0m\n",
      "\u001b[31m[Epoch 5 Batch 400] Training: accuracy=0.985112, 5649.350789 samples/s\u001b[0m\n",
      "\u001b[31m[Epoch 5 Batch 500] Training: accuracy=0.984611, 5640.158677 samples/s\u001b[0m\n",
      "\u001b[31m[Epoch 5] Training: accuracy=0.984717\u001b[0m\n",
      "\u001b[31m[Epoch 5] Validation: accuracy=0.972200\u001b[0m\n",
      "\u001b[31m[Epoch 6 Batch 100] Training: accuracy=0.987030, 6086.463896 samples/s\u001b[0m\n",
      "\u001b[31m[Epoch 6 Batch 200] Training: accuracy=0.987662, 6221.249203 samples/s\u001b[0m\n",
      "\u001b[31m[Epoch 6 Batch 300] Training: accuracy=0.987176, 6518.461419 samples/s\u001b[0m\n",
      "\u001b[31m[Epoch 6 Batch 400] Training: accuracy=0.987357, 7667.551461 samples/s\u001b[0m\n",
      "\u001b[31m[Epoch 6 Batch 500] Training: accuracy=0.987106, 7332.955698 samples/s\u001b[0m\n",
      "\u001b[31m[Epoch 6] Training: accuracy=0.986717\u001b[0m\n",
      "\u001b[31m[Epoch 6] Validation: accuracy=0.976200\u001b[0m\n",
      "\u001b[31m[Epoch 7 Batch 100] Training: accuracy=0.989703, 7691.736659 samples/s\u001b[0m\n",
      "\u001b[31m[Epoch 7 Batch 200] Training: accuracy=0.988806, 6485.502227 samples/s\u001b[0m\n",
      "\u001b[31m[Epoch 7 Batch 300] Training: accuracy=0.988439, 6246.171258 samples/s\u001b[0m\n",
      "\u001b[31m[Epoch 7 Batch 400] Training: accuracy=0.988354, 7083.656753 samples/s\u001b[0m\n",
      "\u001b[31m[Epoch 7 Batch 500] Training: accuracy=0.988004, 5514.757547 samples/s\u001b[0m\n",
      "\u001b[31m[Epoch 7] Training: accuracy=0.987533\u001b[0m\n",
      "\u001b[31m[Epoch 7] Validation: accuracy=0.977200\u001b[0m\n",
      "\u001b[31m[Epoch 8 Batch 100] Training: accuracy=0.991881, 6253.528351 samples/s\u001b[0m\n",
      "\u001b[31m[Epoch 8 Batch 200] Training: accuracy=0.991343, 6517.144722 samples/s\u001b[0m\n",
      "\u001b[31m[Epoch 8 Batch 300] Training: accuracy=0.990997, 7579.154319 samples/s\u001b[0m\n",
      "\u001b[31m[Epoch 8 Batch 400] Training: accuracy=0.990000, 5788.999765 samples/s\u001b[0m\n",
      "\u001b[31m[Epoch 8 Batch 500] Training: accuracy=0.989062, 5746.094200 samples/s\u001b[0m\n",
      "\u001b[31m[Epoch 8] Training: accuracy=0.988833\u001b[0m\n",
      "\u001b[31m[Epoch 8] Validation: accuracy=0.974100\u001b[0m\n",
      "\u001b[31m[Epoch 9 Batch 100] Training: accuracy=0.990792, 6129.693391 samples/s\u001b[0m\n",
      "\u001b[31m[Epoch 9 Batch 200] Training: accuracy=0.991343, 5878.903918 samples/s\u001b[0m\n",
      "\u001b[31m[Epoch 9 Batch 300] Training: accuracy=0.990864, 5918.223251 samples/s\u001b[0m\n",
      "\u001b[31m[Epoch 9 Batch 400] Training: accuracy=0.990249, 7375.767594 samples/s\u001b[0m\n",
      "\u001b[31m[Epoch 9 Batch 500] Training: accuracy=0.989701, 6919.466807 samples/s\u001b[0m\n",
      "\u001b[31m[Epoch 9] Training: accuracy=0.989400\u001b[0m\n",
      "\u001b[31m[Epoch 9] Validation: accuracy=0.976300\u001b[0m\n",
      "\n",
      "2019-05-30 03:22:35 Uploading - Uploading generated training model\n",
      "2019-05-30 03:22:35 Completed - Training job completed\n",
      "Billable seconds: 154\n"
     ]
    }
   ],
   "source": [
    "m = MXNet(\"mnist.py\",\n",
    "          role=role,\n",
    "          train_instance_count=1,\n",
    "          train_instance_type=\"ml.c4.xlarge\",\n",
    "          hyperparameters={'batch_size': 100,\n",
    "                         'epochs': 10,\n",
    "                         'learning_rate': 0.1,\n",
    "                         'momentum': 0.9,\n",
    "                         'log_interval': 100},\n",
    "          framework_version=1.2)\n",
    "m.fit(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training, use the `MXNet` object to build and deploy an `MXNetPredictor` object. This creates an Amazon SageMaker endpoint that you can use to perform inference on JSON-encoded multidimensional arrays.\n",
    "\n",
    "The `deploy` method does the following, in order:\n",
    "\n",
    "1. Creates an Amazon SageMaker model by calling the `CreateModel` API. The model that you create in Amazon SageMaker holds information such as location of the model artifacts and the inference code image.\n",
    "1. Creates an endpoint configuration by calling the `CreateEndpointConfig` API. This configuration holds necessary information including the name of the model (which was created in the preceding step) and the resource configuration (the type and number of Machine Learning compute instances to launch for hosting).\n",
    "1. Creates the endpoint by calling the `CreateEndpoint` API and specifying the endpoint configuration created in the preceding step. Amazon SageMaker launches Machine Learning compute instances as specified in the endpoint configuration, and deploys the model on them.\n",
    "\n",
    "Launching an endpoint can take up to 10 minutes to complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The Python 2 mxnet images will be soon deprecated and may not be supported for newer upcoming versions of the mxnet images.\n",
      "Please set the argument \"py_version='py3'\" to use the Python 3 mxnet image.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------!"
     ]
    }
   ],
   "source": [
    "predictor = m.deploy(initial_instance_count=1, instance_type='ml.m4.xlarge')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now use this predictor to classify hand-written digits. Drawing into the image box loads the pixel data into a `data` variable in this notebook, which can then be passed to the MXNet predictor. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script type=\"text/Javascript\">\n",
       "    var pixels = [];\n",
       "    for (var i = 0; i < 28*28; i++) pixels[i] = 0;\n",
       "    var click = 0;\n",
       "\n",
       "    var canvas = document.querySelector(\"canvas\");\n",
       "    canvas.addEventListener(\"mousemove\", function(e){\n",
       "        if (e.buttons == 1) {\n",
       "            click = 1;\n",
       "            canvas.getContext(\"2d\").fillStyle = \"rgb(0,0,0)\";\n",
       "            canvas.getContext(\"2d\").fillRect(e.offsetX, e.offsetY, 8, 8);\n",
       "            x = Math.floor(e.offsetY * 0.2);\n",
       "            y = Math.floor(e.offsetX * 0.2) + 1;\n",
       "            for (var dy = 0; dy < 2; dy++){\n",
       "                for (var dx = 0; dx < 2; dx++){\n",
       "                    if ((x + dx < 28) && (y + dy < 28)){\n",
       "                        pixels[(y+dy)+(x+dx)*28] = 1;\n",
       "                    }\n",
       "                }\n",
       "            }\n",
       "        } else {\n",
       "            if (click == 1) set_value();\n",
       "            click = 0;\n",
       "        }\n",
       "    });\n",
       "    function clear_value(){\n",
       "        canvas.getContext(\"2d\").fillStyle = \"rgb(255,255,255)\";\n",
       "        canvas.getContext(\"2d\").fillRect(0, 0, 140, 140);\n",
       "        for (var i = 0; i < 28*28; i++) pixels[i] = 0;\n",
       "    }\n",
       "    \n",
       "    function set_value(){\n",
       "        var result = \"[[\"\n",
       "        for (var i = 0; i < 28; i++) {\n",
       "            result += \"[\"\n",
       "            for (var j = 0; j < 28; j++) {\n",
       "                result += pixels [i * 28 + j]\n",
       "                if (j < 27) {\n",
       "                    result += \", \"\n",
       "                }\n",
       "            }\n",
       "            result += \"]\"\n",
       "            if (i < 27) {\n",
       "                result += \", \"\n",
       "            }\n",
       "        }\n",
       "        result += \"]]\"\n",
       "        var kernel = IPython.notebook.kernel;\n",
       "        kernel.execute(\"data = \" + result)\n",
       "    }\n",
       "</script>\n",
       "<table>\n",
       "<td style=\"border-style: none;\">\n",
       "<div style=\"border: solid 2px #666; width: 143px; height: 144px;\">\n",
       "<canvas width=\"140\" height=\"140\"></canvas>\n",
       "</div></td>\n",
       "<td style=\"border-style: none;\">\n",
       "<button onclick=\"clear_value()\">Clear</button>\n",
       "</td>\n",
       "</table>\n",
       "\n",
       "<!-- This work has been modified from the original and is licensed under the Apache 2.0 License. -->\n",
       "\n",
       "<!--\n",
       "                                     Apache License\n",
       "                           Version 2.0, January 2004\n",
       "                        http://www.apache.org/licenses/\n",
       "\n",
       "   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n",
       "\n",
       "   1. Definitions.\n",
       "\n",
       "      \"License\" shall mean the terms and conditions for use, reproduction,\n",
       "      and distribution as defined by Sections 1 through 9 of this document.\n",
       "\n",
       "      \"Licensor\" shall mean the copyright owner or entity authorized by\n",
       "      the copyright owner that is granting the License.\n",
       "\n",
       "      \"Legal Entity\" shall mean the union of the acting entity and all\n",
       "      other entities that control, are controlled by, or are under common\n",
       "      control with that entity. For the purposes of this definition,\n",
       "      \"control\" means (i) the power, direct or indirect, to cause the\n",
       "      direction or management of such entity, whether by contract or\n",
       "      otherwise, or (ii) ownership of fifty percent (50%) or more of the\n",
       "      outstanding shares, or (iii) beneficial ownership of such entity.\n",
       "\n",
       "      \"You\" (or \"Your\") shall mean an individual or Legal Entity\n",
       "      exercising permissions granted by this License.\n",
       "\n",
       "      \"Source\" form shall mean the preferred form for making modifications,\n",
       "      including but not limited to software source code, documentation\n",
       "      source, and configuration files.\n",
       "\n",
       "      \"Object\" form shall mean any form resulting from mechanical\n",
       "      transformation or translation of a Source form, including but\n",
       "      not limited to compiled object code, generated documentation,\n",
       "      and conversions to other media types.\n",
       "\n",
       "      \"Work\" shall mean the work of authorship, whether in Source or\n",
       "      Object form, made available under the License, as indicated by a\n",
       "      copyright notice that is included in or attached to the work\n",
       "      (an example is provided in the Appendix below).\n",
       "\n",
       "      \"Derivative Works\" shall mean any work, whether in Source or Object\n",
       "      form, that is based on (or derived from) the Work and for which the\n",
       "      editorial revisions, annotations, elaborations, or other modifications\n",
       "      represent, as a whole, an original work of authorship. For the purposes\n",
       "      of this License, Derivative Works shall not include works that remain\n",
       "      separable from, or merely link (or bind by name) to the interfaces of,\n",
       "      the Work and Derivative Works thereof.\n",
       "\n",
       "      \"Contribution\" shall mean any work of authorship, including\n",
       "      the original version of the Work and any modifications or additions\n",
       "      to that Work or Derivative Works thereof, that is intentionally\n",
       "      submitted to Licensor for inclusion in the Work by the copyright owner\n",
       "      or by an individual or Legal Entity authorized to submit on behalf of\n",
       "      the copyright owner. For the purposes of this definition, \"submitted\"\n",
       "      means any form of electronic, verbal, or written communication sent\n",
       "      to the Licensor or its representatives, including but not limited to\n",
       "      communication on electronic mailing lists, source code control systems,\n",
       "      and issue tracking systems that are managed by, or on behalf of, the\n",
       "      Licensor for the purpose of discussing and improving the Work, but\n",
       "      excluding communication that is conspicuously marked or otherwise\n",
       "      designated in writing by the copyright owner as \"Not a Contribution.\"\n",
       "\n",
       "      \"Contributor\" shall mean Licensor and any individual or Legal Entity\n",
       "      on behalf of whom a Contribution has been received by Licensor and\n",
       "      subsequently incorporated within the Work.\n",
       "\n",
       "   2. Grant of Copyright License. Subject to the terms and conditions of\n",
       "      this License, each Contributor hereby grants to You a perpetual,\n",
       "      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n",
       "      copyright license to reproduce, prepare Derivative Works of,\n",
       "      publicly display, publicly perform, sublicense, and distribute the\n",
       "      Work and such Derivative Works in Source or Object form.\n",
       "\n",
       "   3. Grant of Patent License. Subject to the terms and conditions of\n",
       "      this License, each Contributor hereby grants to You a perpetual,\n",
       "      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n",
       "      (except as stated in this section) patent license to make, have made,\n",
       "      use, offer to sell, sell, import, and otherwise transfer the Work,\n",
       "      where such license applies only to those patent claims licensable\n",
       "      by such Contributor that are necessarily infringed by their\n",
       "      Contribution(s) alone or by combination of their Contribution(s)\n",
       "      with the Work to which such Contribution(s) was submitted. If You\n",
       "      institute patent litigation against any entity (including a\n",
       "      cross-claim or counterclaim in a lawsuit) alleging that the Work\n",
       "      or a Contribution incorporated within the Work constitutes direct\n",
       "      or contributory patent infringement, then any patent licenses\n",
       "      granted to You under this License for that Work shall terminate\n",
       "      as of the date such litigation is filed.\n",
       "\n",
       "   4. Redistribution. You may reproduce and distribute copies of the\n",
       "      Work or Derivative Works thereof in any medium, with or without\n",
       "      modifications, and in Source or Object form, provided that You\n",
       "      meet the following conditions:\n",
       "\n",
       "      (a) You must give any other recipients of the Work or\n",
       "          Derivative Works a copy of this License; and\n",
       "\n",
       "      (b) You must cause any modified files to carry prominent notices\n",
       "          stating that You changed the files; and\n",
       "\n",
       "      (c) You must retain, in the Source form of any Derivative Works\n",
       "          that You distribute, all copyright, patent, trademark, and\n",
       "          attribution notices from the Source form of the Work,\n",
       "          excluding those notices that do not pertain to any part of\n",
       "          the Derivative Works; and\n",
       "\n",
       "      (d) If the Work includes a \"NOTICE\" text file as part of its\n",
       "          distribution, then any Derivative Works that You distribute must\n",
       "          include a readable copy of the attribution notices contained\n",
       "          within such NOTICE file, excluding those notices that do not\n",
       "          pertain to any part of the Derivative Works, in at least one\n",
       "          of the following places: within a NOTICE text file distributed\n",
       "          as part of the Derivative Works; within the Source form or\n",
       "          documentation, if provided along with the Derivative Works; or,\n",
       "          within a display generated by the Derivative Works, if and\n",
       "          wherever such third-party notices normally appear. The contents\n",
       "          of the NOTICE file are for informational purposes only and\n",
       "          do not modify the License. You may add Your own attribution\n",
       "          notices within Derivative Works that You distribute, alongside\n",
       "          or as an addendum to the NOTICE text from the Work, provided\n",
       "          that such additional attribution notices cannot be construed\n",
       "          as modifying the License.\n",
       "\n",
       "      You may add Your own copyright statement to Your modifications and\n",
       "      may provide additional or different license terms and conditions\n",
       "      for use, reproduction, or distribution of Your modifications, or\n",
       "      for any such Derivative Works as a whole, provided Your use,\n",
       "      reproduction, and distribution of the Work otherwise complies with\n",
       "      the conditions stated in this License.\n",
       "\n",
       "   5. Submission of Contributions. Unless You explicitly state otherwise,\n",
       "      any Contribution intentionally submitted for inclusion in the Work\n",
       "      by You to the Licensor shall be under the terms and conditions of\n",
       "      this License, without any additional terms or conditions.\n",
       "      Notwithstanding the above, nothing herein shall supersede or modify\n",
       "      the terms of any separate license agreement you may have executed\n",
       "      with Licensor regarding such Contributions.\n",
       "\n",
       "   6. Trademarks. This License does not grant permission to use the trade\n",
       "      names, trademarks, service marks, or product names of the Licensor,\n",
       "      except as required for reasonable and customary use in describing the\n",
       "      origin of the Work and reproducing the content of the NOTICE file.\n",
       "\n",
       "   7. Disclaimer of Warranty. Unless required by applicable law or\n",
       "      agreed to in writing, Licensor provides the Work (and each\n",
       "      Contributor provides its Contributions) on an \"AS IS\" BASIS,\n",
       "      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n",
       "      implied, including, without limitation, any warranties or conditions\n",
       "      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n",
       "      PARTICULAR PURPOSE. You are solely responsible for determining the\n",
       "      appropriateness of using or redistributing the Work and assume any\n",
       "      risks associated with Your exercise of permissions under this License.\n",
       "\n",
       "   8. Limitation of Liability. In no event and under no legal theory,\n",
       "      whether in tort (including negligence), contract, or otherwise,\n",
       "      unless required by applicable law (such as deliberate and grossly\n",
       "      negligent acts) or agreed to in writing, shall any Contributor be\n",
       "      liable to You for damages, including any direct, indirect, special,\n",
       "      incidental, or consequential damages of any character arising as a\n",
       "      result of this License or out of the use or inability to use the\n",
       "      Work (including but not limited to damages for loss of goodwill,\n",
       "      work stoppage, computer failure or malfunction, or any and all\n",
       "      other commercial damages or losses), even if such Contributor\n",
       "      has been advised of the possibility of such damages.\n",
       "\n",
       "   9. Accepting Warranty or Additional Liability. While redistributing\n",
       "      the Work or Derivative Works thereof, You may choose to offer,\n",
       "      and charge a fee for, acceptance of support, warranty, indemnity,\n",
       "      or other liability obligations and/or rights consistent with this\n",
       "      License. However, in accepting such obligations, You may act only\n",
       "      on Your own behalf and on Your sole responsibility, not on behalf\n",
       "      of any other Contributor, and only if You agree to indemnify,\n",
       "      defend, and hold each Contributor harmless for any liability\n",
       "      incurred by, or claims asserted against, such Contributor by reason\n",
       "      of your accepting any such warranty or additional liability.\n",
       "\n",
       "   END OF TERMS AND CONDITIONS\n",
       "\n",
       "   APPENDIX: How to apply the Apache License to your work.\n",
       "\n",
       "      To apply the Apache License to your work, attach the following\n",
       "      boilerplate notice, with the fields enclosed by brackets \"{}\"\n",
       "      replaced with your own identifying information. (Don't include\n",
       "      the brackets!)  The text should be enclosed in the appropriate\n",
       "      comment syntax for the file format. We also recommend that a\n",
       "      file or class name and description of purpose be included on the\n",
       "      same \"printed page\" as the copyright notice for easier\n",
       "      identification within third-party archives.\n",
       "\n",
       "   Copyright {yyyy} {name of copyright owner}\n",
       "\n",
       "   Licensed under the Apache License, Version 2.0 (the \"License\");\n",
       "   you may not use this file except in compliance with the License.\n",
       "   You may obtain a copy of the License at\n",
       "\n",
       "       http://www.apache.org/licenses/LICENSE-2.0\n",
       "\n",
       "   Unless required by applicable law or agreed to in writing, software\n",
       "   distributed under the License is distributed on an \"AS IS\" BASIS,\n",
       "   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
       "   See the License for the specific language governing permissions and\n",
       "   limitations under the License.\n",
       "-->\n",
       "\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML(open(\"input.html\").read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The predictor runs inference on your input data and returns the predicted digit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "response = predictor.predict(data)\n",
    "print(int(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleanup\n",
    "\n",
    "After you have finished with this example, remember to delete the prediction endpoint to release the instance(s) associated with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker.Session().delete_endpoint(predictor.endpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"s3p2\"></a>\n",
    "### Challenge Option 2: Deploying from Amazon S3\n",
    "\n",
    "\n",
    "Amazon SageMaker saves and stores the artifacts in Amazon S3. In the case where you have already trained a model elsewhere, it is relatively straightforward to upload the model artifacts to Amazon S3 and deploy the model using Amazon SageMaker.\n",
    "\n",
    "In this case, use the model trained in the previous section. You will create a model using the [sagemaker.mxnet.model.MXNetModel](http://sagemaker.readthedocs.io/en/latest/sagemaker.mxnet.html#mxnet-model) function, passing the location in Amazon S3 with the zipped and archived copy of the model artifacts, and then deploy it using Amazon S3.\n",
    "\n",
    "#### Locate the model artifacts in S3\n",
    "\n",
    "The model in the previous section saved the artifacts including the parameters and model declaration in Amazon S3. This will be in the default bucket used by the notebook (`sagemaker-REGION-ACCOUNT_ID`). Locate this bucket and scroll down to find the latest folder that was added. It will be named the same as the endpoint that was deleted in the code cell above. Inside this folder, under `output`, you will find a `model.tar.gz` file. Download this, unpack it, and have a look at the contents. Here is an example path:\n",
    "\n",
    "`BUCKET_NAME/sagemaker-mxnet-2018-07-20-08-45-48-381/output/model.tar.gz`\n",
    "\n",
    "Two files are contained:\n",
    "\n",
    "* `model.json`: The declaration of the network.\n",
    "* `model.params`: The trained parameters of the model.\n",
    "\n",
    "#### Copy the model artifacts to a new location\n",
    "\n",
    "Copy the `model.tar.gz` into your lab bucket.\n",
    "\n",
    "#### Load the model using MXNetModel\n",
    "\n",
    "Load model from S3 using [sagemaker.mxnet.model.MXNetModel](http://sagemaker.readthedocs.io/en/latest/sagemaker.mxnet.html#mxnet-model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data = \"s3://%s/model.tar.gz\" % bucket\n",
    "\n",
    "m2 = MXNetModel(model_data, role, \"mnist.py\")\n",
    "\n",
    "predictor = m2.deploy(initial_instance_count=1, instance_type='ml.m4.xlarge')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This **may** fail. If so, it is because you already have a model set up with the same configuration. How do you think it knows that it is the same model? Go to the Amazon SageMaker console and find the model under Inference -> Models and copy the name of the model. Replace **MODEL_NAME** in the code cell below with the name of your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################\n",
    "# Only run this cell if the previous code cell failed! #\n",
    "########################################################\n",
    "model_name = 'sagemaker-mxnet-2019-05-30-03-18-06-821'\n",
    "\n",
    "sagemaker_client = boto3.client('sagemaker')\n",
    "sagemaker_client.delete_model(ModelName=model_name)\n",
    "predictor = m2.deploy(initial_instance_count=1, instance_type='ml.m4.xlarge')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the same input you drew in the cell above and score against this new endpoint. If you would like to do so, feel free to clear the input box and try additional numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = predictor.predict(data)\n",
    "print(int(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleanup\n",
    "\n",
    "After you have finished with this example, remember to delete the prediction endpoint to release the instance(s) associated with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker.Session().delete_endpoint(predictor.endpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"s3p3\"></a>\n",
    "### Challenge Option 3: Deploying Using a Custom Docker Container\n",
    "\n",
    "\n",
    "With Amazon SageMaker, you can package your own algorithms that can then be trained and deployed in the Amazon SageMaker environment. This notebook will guide you through an example that shows you how to build a Docker container for Amazon SageMaker and use it for training and inference.\n",
    "\n",
    "By packaging an algorithm in a container, you can bring almost any code to the Amazon SageMaker environment, regardless of programming language, environment, framework, or dependencies.\n",
    "\n",
    "1. [Building your own algorithm container](#Building-your-own-algorithm-container)\n",
    "  1. [When should I build my own algorithm container?](#When-should-I-build-my-own-algorithm-container?)\n",
    "  1. [The example](#The-example)\n",
    "1. [Packaging and Uploading your Algorithm for use with Amazon SageMaker](#Part-1:-Packaging-and-Uploading-your-Algorithm-for-use-with-Amazon-SageMaker)\n",
    "    1. [An overview of Docker](#An-overview-of-Docker)\n",
    "    1. [How Amazon SageMaker runs your Docker container](#How-Amazon-SageMaker-runs-your-Docker-container)\n",
    "      1. [Running your container during training](#Running-your-container-during-training)\n",
    "      1. [Running your container during hosting](#Running-your-container-during-hosting)\n",
    "    1. [The Dockerfile](#The-Dockerfile)\n",
    "    1. [Building and registering the container](#Building-and-registering-the-container)\n",
    "  1. [Testing your algorithm on your local machine or on an Amazon SageMaker notebook instance](#Testing-your-algorithm-on-your-local-machine-or-on-an-Amazon-SageMaker-notebook-instance)\n",
    "1. [Part 2: Training and Hosting your Algorithm in Amazon SageMaker](#Part-2:-Training-and-Hosting-your-Algorithm-in-Amazon-SageMaker)\n",
    "  1. [Set up the environment](#Set-up-the-environment)\n",
    "  1. [Create the session](#Create-the-session)\n",
    "  1. [Upload the data for training](#Upload-the-data-for-training)\n",
    "  1. [Create an estimator and fit the model](#Create-an-estimator-and-fit-the-model)\n",
    "  1. [Deploy the model](#Deploy-the-model)\n",
    "  1. [Choose some data and use it for a prediction](#Choose-some-data-and-use-it-for-a-prediction)\n",
    "  1. [Optional cleanup](#Optional-cleanup)  \n",
    "\n",
    "_or_ if you're feeling a little impatient, you can jump directly [to the code](#The-Dockerfile)!\n",
    "\n",
    "#### Building your Own Algorithm Container\n",
    "<a name=\"Building-your-own-algorithm-container\"></a>\n",
    "\n",
    "##### When Should I Build my Own Algorithm Container?\n",
    "<a name=\"When-should-I-build-my-own-algorithm-container?\"></a>\n",
    "\n",
    "You may not need to create a container to bring your own code to Amazon SageMaker. When you are using a framework (such as Apache MXNet or TensorFlow) that has direct support in Amazon SageMaker, you can simply supply the Python code that implements your algorithm using the SDK entry points for that framework. This set of frameworks is continually expanding, so we recommend that you check the current list if your algorithm is written in a common machine learning environment.\n",
    "\n",
    "Even if there is direct SDK support for your environment or framework, you may find it more effective to build your own container. If the code that implements your algorithm is quite complex on its own or you need special additions to the framework, building your own container may be the right choice.\n",
    "\n",
    "If there isn't direct SDK support for your environment, don't worry. You'll see in this walk-through that building your own container is quite straightforward.\n",
    "\n",
    "##### The Example\n",
    "<a name=\"The-example\"></a>\n",
    "\n",
    "Here, you'll see how to package a simple Python example which showcases the decision tree algorithm from the widely used `scikit-learn` machine learning package. The example is purposefully fairly trivial since the point is to show the surrounding structure that you'll want to add to your own code so you can train and host it in Amazon SageMaker.\n",
    "\n",
    "The ideas shown here will work in any language or environment. You'll need to choose the right tools for your environment to serve HTTP requests for inference, but good HTTP environments are available in every language these days.\n",
    "\n",
    "In this example, you will use a single image to support training and hosting. This is easy because it means that you only need to manage one image and can set it up to do everything. Sometimes you'll want separate images for training and hosting because they have different requirements. Just separate the parts discussed below into separate Dockerfiles and build two images. Choosing whether to have a single image or two images is really a matter of which is more convenient for you to develop and manage.\n",
    "\n",
    "If you're only using Amazon SageMaker for training or hosting, but not both, there is no need to build the unused functionality into your container.\n",
    "\n",
    "[scikit-learn]: http://scikit-learn.org/stable/\n",
    "[decision tree]: http://scikit-learn.org/stable/modules/tree.html\n",
    "\n",
    "#### Packaging and Uploading your Algorithm for use with Amazon SageMaker\n",
    "<a name=\"Part-1:-Packaging-and-Uploading-your-Algorithm-for-use-with-Amazon-SageMaker\"></a>\n",
    "\n",
    "##### An Overview of Docker\n",
    "<a name=\"An-overview-of-Docker\"></a>\n",
    "\n",
    "If you're familiar with Docker already, you can skip ahead to the next section.\n",
    "\n",
    "For many data scientists, Docker containers are a new concept, but they are not difficult, as you'll see here.\n",
    "\n",
    "Docker provides a simple way to package arbitrary code into an _image_ that is totally self-contained. Once you have an image, you can use Docker to run a _container_ based on that image. Running a container is just like running a program on the machine except that the container creates a fully self-contained environment for the program to run. Containers are isolated from each other and from the host environment, so the way you set up your program is the way it runs, no matter where you run it.\n",
    "\n",
    "Docker is more powerful than environment managers like conda or virtualenv because (a) it is completely language independent and (b) it comprises your whole operating environment, including startup commands, environment variable, etc.\n",
    "\n",
    "In some ways, a Docker container is like a virtual machine, but it is much lighter weight. For example, a program running in a container can start in less than a second and many containers can run on the same physical machine or virtual machine instance.\n",
    "\n",
    "Docker uses a simple file called a `Dockerfile` to specify how the image is assembled. We'll see an example of that below. You can build your Docker images based on Docker images built by yourself or others, which can simplify things quite a bit.\n",
    "\n",
    "Docker has become very popular in the programming and devops communities for its flexibility and well-defined specification of the code to be run. It is the underpinning of many services built in the past few years, such as Amazon Elastic Container Service (ECS).\n",
    "\n",
    "Amazon SageMaker uses Docker to allow users to train and deploy arbitrary algorithms.\n",
    "\n",
    "In Amazon SageMaker, Docker containers are invoked in a certain way for training and a slightly different way for hosting. The following sections outline how to build containers for the Amazon SageMaker environment.\n",
    "\n",
    "Some helpful links:\n",
    "\n",
    "* [Docker home page](http://www.docker.com)\n",
    "* [Getting started with Docker](https://docs.docker.com/get-started/)\n",
    "* [Dockerfile reference](https://docs.docker.com/engine/reference/builder/)\n",
    "* [`docker run` reference](https://docs.docker.com/engine/reference/run/)\n",
    "\n",
    "[Amazon ECS]: https://aws.amazon.com/ecs/\n",
    "\n",
    "##### How Amazon SageMaker Runs your Docker Container\n",
    "<a name=\"How-Amazon-SageMaker-runs-your-Docker-container\"></a>\n",
    "\n",
    "Because you can run the same image in training or hosting, Amazon SageMaker runs your container with the argument `train` or `serve`. How your container processes this argument depends on the container:\n",
    "\n",
    "* In the example here, an `ENTRYPOINT` is not defined in the Dockerfile so Docker will run the command `train` at training time and `serve` at serving time. In this example, these are defined as executable Python scripts, but they could be any program that you want to start in that environment.\n",
    "* If you specify a program as an `ENTRYPOINT` in the Dockerfile, that program will be run at startup and its first argument will be `train` or `serve`. The program can then look at that argument and decide what to do.\n",
    "* If you are building separate containers for training and hosting (or building only for one or the other), you can define a program as an `ENTRYPOINT` in the Dockerfile and ignore (or verify) the first argument passed in.\n",
    "\n",
    "###### Running your Container During Training\n",
    "<a name=\"Running-your-container-during-training\"></a>\n",
    "\n",
    "When Amazon SageMaker runs training, your `train` script is run just like a regular Python program. A number of files are laid out for your use, under the `/opt/ml` directory:\n",
    "\n",
    "    ```plain\n",
    "    /opt/ml\n",
    "    ├── input\n",
    "    │   ├── config\n",
    "    │   │   ├── hyperparameters.json\n",
    "    │   │   └── resourceConfig.json\n",
    "    │   └── data\n",
    "    │       └── <channel_name>\n",
    "    │           └── <input data>\n",
    "    ├── model\n",
    "    │   └── <model files>\n",
    "    └── output\n",
    "        └── failure\n",
    "    ```\n",
    "\n",
    "**The Input**\n",
    "\n",
    "* `/opt/ml/input/config` contains information to control how your program runs.\n",
    "  * `hyperparameters.json` is a JSON-formatted dictionary of hyperparameter names to values. These values will always be strings, so you may need to convert them.\n",
    "  * `resourceConfig.json` is a JSON-formatted file that describes the network layout used for distributed training. Since scikit-learn doesn't support distributed training, ignore it here.\n",
    "* `/opt/ml/input/data/<channel_name>/` (for File mode) contains the input data for that channel. The channels are created based on the call to `CreateTrainingJob` but it's generally important that channels match what the algorithm expects. The files for each channel will be copied from Amazon S3 to this directory, preserving the tree structure indicated by the S3 key structure.\n",
    "* `/opt/ml/input/data/<channel_name>_<epoch_number>` (for Pipe mode) is the pipe for a given epoch. Epochs start at zero and go up by one each time you read them. There is no limit to the number of epochs that you can run, but you must close each pipe before reading the next epoch.\n",
    "\n",
    "**The Output**\n",
    "\n",
    "* `/opt/ml/model/` is the directory where you write the model that your algorithm generates. Your model can be in any format that you want. It can be a single file or a whole directory tree. Amazon SageMaker will package any files in this directory into a compressed tar archive file. This file will be available at the S3 location returned in the `DescribeTrainingJob` result.\n",
    "* `/opt/ml/output` is a directory where the algorithm can write a file `failure` that describes why the job failed. The contents of this file will be returned in the `FailureReason` field of the `DescribeTrainingJob` result. For jobs that succeed, there is no reason to write this file as it will be ignored.\n",
    "\n",
    "###### Running your Container During Hosting\n",
    "<a name=\"Running-your-container-during-hosting\"></a>\n",
    "\n",
    "Hosting has a very different model than training because hosting is reponding to inference requests that come in via HTTP. In this example, you will use a Python serving stack to provide robust and scalable serving of inference requests:\n",
    "\n",
    "![](./stack.png)\n",
    "\n",
    "This stack is implemented in the sample code here and you can mostly just leave it alone.\n",
    "\n",
    "Amazon SageMaker uses two URLs in the container:\n",
    "\n",
    "* `/ping` will receive `GET` requests from the infrastructure. Your program returns `200` if the container is up and accepting requests.\n",
    "* `/invocations` is the endpoint that receives client inference `POST` requests. The format of the request and the response is up to the algorithm. If the client supplied `ContentType` and `Accept` headers, these will be passed in as well.\n",
    "\n",
    "The container will have the model files in the same place they were written during training:\n",
    "\n",
    "    ```plain\n",
    "    /opt/ml\n",
    "    └── model\n",
    "        └── <model files>\n",
    "    ```\n",
    "\n",
    "##### The Dockerfile\n",
    "<a name=\"The-Dockerfile\"></a>\n",
    "\n",
    "The Dockerfile describes the image that you want to build. You can think of it as describing the complete operating system installation of the system that you want to run. A Docker container running is quite a bit lighter than a full operating system, however, because it takes advantage of Linux on the host machine for the basic operations.\n",
    "\n",
    "For the Python science stack, start from a standard Ubuntu installation and run the normal tools to install the things needed by `scikit-learn`. Finally, add the code that implements your specific algorithm to the container and set up the right environment to run under.\n",
    "\n",
    "Along the way, clean up extra space. This makes the container smaller and faster to start.\n",
    "\n",
    "Look at the Dockerfile for the example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat container/Dockerfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Building and Registering the Container\n",
    "<a name=\"Building-and-registering-the-container\"></a>\n",
    "\n",
    "The following shell code shows how to build the container image using `docker build` and push the container image to Amazon ECR using `docker push`.\n",
    "\n",
    "This code looks for an Amazon ECR repository in the account you're using and the current default region (if you're using a Amazon SageMaker notebook instance, this will be the region where the notebook instance was created). If the repository doesn't exist, the script will create it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "\n",
    "# The name of our algorithm\n",
    "algorithm_name=decision-trees-sample\n",
    "\n",
    "cd container\n",
    "\n",
    "chmod +x decision_trees/train\n",
    "chmod +x decision_trees/serve\n",
    "\n",
    "account=$(aws sts get-caller-identity --query Account --output text)\n",
    "\n",
    "# Get the region defined in the current configuration (default to us-west-2 if none defined)\n",
    "region=$(aws configure get region)\n",
    "region=${region:-us-west-2}\n",
    "\n",
    "fullname=\"${account}.dkr.ecr.${region}.amazonaws.com/${algorithm_name}:latest\"\n",
    "\n",
    "# If the repository doesn't exist in ECR, create it.\n",
    "\n",
    "aws ecr describe-repositories --repository-names \"${algorithm_name}\" > /dev/null 2>&1\n",
    "\n",
    "if [ $? -ne 0 ]\n",
    "then\n",
    "    aws ecr create-repository --repository-name \"${algorithm_name}\" > /dev/null\n",
    "fi\n",
    "\n",
    "# Get the login command from ECR and execute it directly\n",
    "$(aws ecr get-login --region ${region} --no-include-email)\n",
    "\n",
    "# Build the docker image locally with the image name and then push it to ECR\n",
    "# with the full name.\n",
    "\n",
    "docker build  -t ${algorithm_name} .\n",
    "docker tag ${algorithm_name} ${fullname}\n",
    "\n",
    "docker push ${fullname}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Testing your Algorithm on your Local Machine or on an Amazon SageMaker Notebook Instance\n",
    "<a name=\"Testing-your-algorithm-on-your-local-machine-or-on-an-Amazon-SageMaker-notebook-instance\"></a>\n",
    "\n",
    "While you're first packaging an algorithm to use with Amazon SageMaker, you probably want to test it yourself to make sure it's working right. In the directory `local_test`, there is a framework for doing this. It includes three shell scripts for running and using the container and a directory structure that mimics the one outlined above.\n",
    "\n",
    "The scripts are:\n",
    "\n",
    "* `train_local.sh`: Run this with the name of the image and it will run training on the local tree. You'll want to modify the directory `test_dir/input/data/...` to be set up with the correct channels and data for your algorithm. Also, you'll want to modify the file `input/config/hyperparameters.json` to have the hyperparameter settings that you want to test (as strings).\n",
    "* `serve_local.sh`: Run this with the name of the image once you've trained the model and it should serve the model. It will run and wait for requests. Simply use the keyboard interrupt to stop it.\n",
    "* `predict.sh`: Run this with the name of a payload file and (optionally) the HTTP content type you want. The content type will default to `text/csv`. For example, you can run `$ ./predict.sh payload.csv text/csv`. To test predict pass a csv file with 4 columns of floats. A single row will work.\n",
    "\n",
    "The directories as shipped are set up to test the decision tree's sample algorithm presented here.\n",
    "\n",
    "#### Training and Hosting your Algorithm in Amazon SageMaker\n",
    "<a name=\"Part-2:-Training-and-Hosting-your-Algorithm-in-Amazon-SageMaker\"></a>\n",
    "\n",
    "Once you have your container packaged, you can use it to train and serve models. Do that with the algorithm you made above.\n",
    "\n",
    "##### Set up the Environment\n",
    "<a name=\"Set-up-the-environment\"></a>\n",
    "\n",
    "Here you specify a bucket prefix to use and the role that will be used for working with Amazon SageMaker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# S3 prefix\n",
    "prefix = 'DEMO-scikit-byo-iris'\n",
    "\n",
    "import boto3\n",
    "import re\n",
    "import itertools\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sagemaker import get_execution_role\n",
    "from time import gmtime, strftime\n",
    "\n",
    "role = get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create the Session\n",
    "<a name=\"Create-the-session\"></a>\n",
    "\n",
    "The session remembers your connection parameters to Amazon SageMaker. You will use it to perform all of your Amazon SageMaker operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = sagemaker.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Upload the Data for Training\n",
    "<a name=\"Upload-the-data-for-training\"></a>\n",
    "\n",
    "When training large models with huge amounts of data, you'll typically use big data tools, like Amazon Athena, AWS Glue, or Amazon EMR, to create your data in S3. For the purposes of this example, you're using some of the classic [Iris dataset](https://en.wikipedia.org/wiki/Iris_flower_data_set), which has been included.\n",
    "\n",
    "We can use the tools provided by the Amazon SageMaker Python SDK to upload the data to a default bucket. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORK_DIRECTORY = 'iris_data'\n",
    "data_location = session.upload_data(WORK_DIRECTORY, key_prefix=prefix)\n",
    "print(data_location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create an Estimator and Fit the Model\n",
    "<a name=\"Create-an-estimator-and-fit-the-model\"></a>\n",
    "\n",
    "In order to use Amazon SageMaker to fit your algorithm, you'll create an `Estimator` that defines how to use the container to train. This includes the configuration needed to invoke Amazon SageMaker training:\n",
    "\n",
    "* The __container name__. This is constructed as in the shell commands above.\n",
    "* The __role__. As defined above.\n",
    "* The __instance count__ which is the number of machines to use for training.\n",
    "* The __instance type__ which is the type of machine to use for training.\n",
    "* The __output path__ determines where the model artifact will be written.\n",
    "* The __session__ is the Amazon SageMaker session object that you defined above.\n",
    "\n",
    "Then you'll use `fit()` on the estimator to train against the data you uploaded previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "account = session.boto_session.client('sts').get_caller_identity()['Account']\n",
    "region = session.boto_session.region_name\n",
    "image = '{}.dkr.ecr.{}.amazonaws.com/decision-trees-sample:latest'.format(account, region)\n",
    "\n",
    "tree = sagemaker.estimator.Estimator(image,\n",
    "                                     role,\n",
    "                                     1,\n",
    "                                     'ml.c4.2xlarge',\n",
    "                                     output_path=\"s3://{}/output\".format(session.default_bucket()),\n",
    "                                     sagemaker_session=session)\n",
    "\n",
    "tree.fit(data_location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Deploy the Model\n",
    "<a name=\"Deploy-the-model\"></a>\n",
    "\n",
    "Deploying the model to Amazon SageMaker hosting just requires a `deploy` call on the fitted model. This call takes an instance count, instance type, and optionally serializer and deserializer functions. These are used when the resulting predictor is created on the endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.predictor import csv_serializer\n",
    "predictor = tree.deploy(1, 'ml.m4.xlarge', serializer=csv_serializer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Choose Some Data and Use it for a Prediction\n",
    "<a name=\"Choose-some-data-and-use-it-for-a-prediction\"></a>\n",
    "\n",
    "In order to do some predictions, extract some of the data used for training and do predictions against it. This is, of course, bad statistical practice, but a good way to see how the mechanism works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape=pd.read_csv(\"iris_data/iris.csv\",header=None)\n",
    "\n",
    "a = [50*i for i in range(3)]\n",
    "b = [40+i for i in range(10)]\n",
    "indices = [i+j for i,j in itertools.product(a,b)]\n",
    "\n",
    "test_data=shape.iloc[indices[:-1]]\n",
    "test_X=test_data.iloc[:,1:]\n",
    "test_y=test_data.iloc[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction is as easy as calling predict with the predictor returned from the `deploy` command and the data you want to do predictions with. The serializers take care of doing the data conversions for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predictor.predict(test_X.values).decode('utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Optional Cleanup\n",
    "<a name=\"Optional-cleanup\"></a>\n",
    "\n",
    "When you're done with the endpoint, you'll want to clean it up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.delete_endpoint(predictor.endpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Break: Instructor Review\n",
    "\n",
    "___\n",
    "\n",
    "Now that you have completed the last challenge-focused aspect of this lab, take a pause and listen to the instructor review some of the key steps you just performed. This will help ensure you fully understand the key aspects of what has been covered in this lab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab Complete\n",
    "\n",
    "Congratulations! You have completed this lab. To clean up your lab environment, do the following:\n",
    "\n",
    "1. To sign out of the AWS Management Console, click **awsstudent** at the top of the console, and then click **Sign Out**.\n",
    "1. On the Qwiklabs page, click **End**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_amazonei_mxnet_p36",
   "language": "python",
   "name": "conda_amazonei_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
